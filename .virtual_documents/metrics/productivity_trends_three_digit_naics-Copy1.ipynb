import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.stattools import grangercausalitytests
import statsmodels.formula.api as smf
from itertools import product


df = pd.read_csv('../processed_data/three_digit_NAICS_final.csv')


num_industries = df['Industry'].nunique()
print(num_industries)


df = df.sort_values(['Industry', 'year'])

df['tfp_ann_pct_lead'] = df.groupby('Industry')['tfp_ann_pct'].shift(-1)
df['tfp_pct_change_lead'] = df.groupby('Industry')['tfp_pct_change'].shift(-1)



pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)





df = df.sort_values(['Industry', 'year'])

mod1 = smf.ols(
    'tfp_ann_pct_lead ~ eer',
    data=df
).fit(cov_type='HC1')          

print(mod1.summary())



df = df.sort_values(['Industry', 'year'])

dependent_vars = [
    'tfp_ann_pct_lead', 
    'tfp_pct_change_lead', 
    'tfp_diff3', 
    'tfp_ann_pct', 
    'tfp_log', 
    'tfp_index_2017'
]

independent_vars = [
    'eer', 
    'pct_high_growth_emp', 
    'reallocation_rate'
]

results = {}

print("="*80)
print("SIMPLE OLS REGRESSION RESULTS")
print("="*80)

for dep_var in dependent_vars:
    for indep_var in independent_vars:
        
        # Create the formula
        formula = f'{dep_var} ~ {indep_var}'
        
        try:
            # Run the regression
            model = smf.ols(formula, data=df).fit(cov_type='HC1')
            
            # Store results
            key = f'{dep_var}_vs_{indep_var}'
            results[key] = model
            
            # Print summary information
            print(f"\n{'-'*60}")
            print(f"MODEL: {dep_var} ~ {indep_var}")
            print(f"{'-'*60}")
            print(f"R-squared: {model.rsquared:.4f}")
            print(f"Adj. R-squared: {model.rsquared_adj:.4f}")
            print(f"F-statistic: {model.fvalue:.4f}")
            print(f"Prob (F-statistic): {model.f_pvalue:.4f}")
            print(f"N observations: {model.nobs:.0f}")
            
            # Print coefficient information
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]  # Robust standard errors
            t_stat = model.tvalues[indep_var]
            p_value = model.pvalues[indep_var]
            
            print(f"\nCoefficient on {indep_var}:")
            print(f"  Estimate: {coef:.6f}")
            print(f"  Std Error: {se:.6f}")
            print(f"  t-statistic: {t_stat:.4f}")
            print(f"  p-value: {p_value:.4f}")
            print(f"  Significance: {'***' if p_value < 0.01 else '**' if p_value < 0.05 else '*' if p_value < 0.10 else ''}")
            
        except Exception as e:
            print(f"\nERROR with {dep_var} ~ {indep_var}: {str(e)}")
            continue

print("\n" + "="*80)
print("SUMMARY TABLE OF ALL REGRESSIONS")
print("="*80)

summary_data = []
for dep_var in dependent_vars:
    for indep_var in independent_vars:
        key = f'{dep_var}_vs_{indep_var}'
        if key in results:
            model = results[key]
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]
            p_val = model.pvalues[indep_var]
            r2 = model.rsquared
            n_obs = model.nobs
            
            summary_data.append({
                'Dependent Variable': dep_var,
                'Independent Variable': indep_var,
                'Coefficient': f"{coef:.6f}",
                'Std Error': f"{se:.6f}",
                'p-value': f"{p_val:.4f}",
                'R-squared': f"{r2:.4f}",
                'N': f"{n_obs:.0f}",
                'Significance': '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*' if p_val < 0.10 else ''
            })

summary_df = pd.DataFrame(summary_data)
print(summary_df.to_string(index=False))

summary_df.to_csv('/Users/danielseymour/Developer/EC334-Summative/processed_data/ols_regression_results.csv', index=False)
print(f"\nResults saved to: /Users/danielseymour/Developer/EC334-Summative/processed_data/ols_regression_results.csv")


data = df[['tfp_ann_pct','pct_high_growth_emp']].dropna()
grangercausalitytests(data[['pct_high_growth_emp','tfp_ann_pct']], maxlag=3)


plot_df = df[['eer', 'tfp_pct_change']].dropna()
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

ax1 = axes[0, 0]
ax1.scatter(plot_df['eer'], plot_df['tfp_pct_change'], alpha=0.5, s=30)
ax1.set_xlabel('Entry-to-Exit Ratio (EER)')
ax1.set_ylabel('TFP % Change')
ax1.set_title('Basic Scatter: EER vs TFP % Change')
ax1.axhline(y=0, color='red', linestyle='--', alpha=0.3)
ax1.axvline(x=1, color='red', linestyle='--', alpha=0.3)


ax2 = axes[0, 1]
ax2.scatter(plot_df['eer'], plot_df['tfp_pct_change'], alpha=0.5, s=30)
# Add regression line
z = np.polyfit(plot_df['eer'], plot_df['tfp_pct_change'], 1)
p = np.poly1d(z)
x_line = np.linspace(plot_df['eer'].min(), plot_df['eer'].max(), 100)
ax2.plot(x_line, p(x_line), "r-", linewidth=2, label=f'y={z[0]:.3f}x+{z[1]:.3f}')
ax2.set_xlabel('Entry-to-Exit Ratio (EER)')
ax2.set_ylabel('TFP % Change')
ax2.set_title('Scatter with OLS Regression Line')
ax2.legend()

ax3 = axes[1, 0]
hexbin = ax3.hexbin(plot_df['eer'], plot_df['tfp_pct_change'], 
                    gridsize=25, cmap='YlOrRd', mincnt=1)
ax3.set_xlabel('Entry-to-Exit Ratio (EER)')
ax3.set_ylabel('TFP % Change')
ax3.set_title('Hexbin Plot: Density of Observations')
plt.colorbar(hexbin, ax=ax3, label='Count')


ax4 = axes[1, 1]
ax4.scatter(plot_df['eer'], plot_df['tfp_pct_change'], alpha=0.3, s=30)
# Add LOWESS smoothing
from statsmodels.nonparametric.smoothers_lowess import lowess
# Sort for smooth line
sorted_df = plot_df.sort_values('eer')
smoothed = lowess(sorted_df['tfp_pct_change'], sorted_df['eer'], frac=0.15)
ax4.plot(smoothed[:, 0], smoothed[:, 1], 'r-', linewidth=2, label='LOWESS smooth')
ax4.set_xlabel('Entry-to-Exit Ratio (EER)')
ax4.set_ylabel('TFP % Change')
ax4.set_title('Scatter with LOWESS Smoothing')
ax4.legend()

plt.tight_layout()
plt.show()


fig2, axes2 = plt.subplots(1, 3, figsize=(15, 5))


ax5 = axes2[0]
df['period'] = pd.cut(df['year'], bins=[1990, 2000, 2010, 2020, 2023], 
                      labels=['1990s', '2000s', '2010s', '2020s'])
for period in df['period'].dropna().unique():
    period_data = df[df['period'] == period]
    ax5.scatter(period_data['eer'], period_data['tfp_pct_change'], 
                alpha=0.5, s=30, label=period)
ax5.set_xlabel('Entry-to-Exit Ratio (EER)')
ax5.set_ylabel('TFP % Change')
ax5.set_title('EER vs TFP by Time Period')
ax5.legend()

ax6 = axes2[1]
# Create EER bins
plot_df['eer_bin'] = pd.cut(plot_df['eer'], bins=10)
plot_df.boxplot(column='tfp_pct_change', by='eer_bin', ax=ax6, rot=45)
ax6.set_xlabel('EER Bins')
ax6.set_ylabel('TFP % Change')
ax6.set_title('TFP % Change Distribution by EER Bins')

ax7 = axes2[2]
industry_counts = df.groupby('Industry').size().sort_values(ascending=False)
top_industries = industry_counts.head(10).index
correlations = []
for ind in top_industries:
    ind_data = df[df['Industry'] == ind][['eer', 'tfp_pct_change']].dropna()
    if len(ind_data) > 10:  # Need enough data points
        corr = ind_data['eer'].corr(ind_data['tfp_pct_change'])
        correlations.append({'Industry': ind, 'Correlation': corr, 'N': len(ind_data)})

corr_df = pd.DataFrame(correlations).sort_values('Correlation')
bars = ax7.barh(range(len(corr_df)), corr_df['Correlation'])
ax7.set_yticks(range(len(corr_df)))
ax7.set_yticklabels([f"{row['Industry'][:20]}... (n={row['N']})" 
                     for _, row in corr_df.iterrows()], fontsize=8)
ax7.set_xlabel('Correlation between EER and TFP % Change')
ax7.set_title('Correlation by Industry (Top 10)')
ax7.axvline(x=0, color='black', linestyle='-', alpha=0.3)

for i, (bar, corr) in enumerate(zip(bars, corr_df['Correlation'])):
    bar.set_color('darkgreen' if corr > 0 else 'darkred')

plt.tight_layout()
plt.show()

print("\n=== Summary Statistics ===")
print(f"Observations plotted: {len(plot_df)}")
print(f"EER range: [{plot_df['eer'].min():.2f}, {plot_df['eer'].max():.2f}]")
print(f"TFP % change range: [{plot_df['tfp_pct_change'].min():.2f}, {plot_df['tfp_pct_change'].max():.2f}]")
print(f"Correlation: {plot_df['eer'].corr(plot_df['tfp_pct_change']):.4f}")

print("\n=== Distribution Check ===")
print(f"% of observations with EER > 1: {(plot_df['eer'] > 1).mean()*100:.1f}%")
print(f"% of observations with positive TFP growth: {(plot_df['tfp_pct_change'] > 0).mean()*100:.1f}%")

eer_outliers = plot_df['eer'] > plot_df['eer'].quantile(0.99)
tfp_outliers = abs(plot_df['tfp_pct_change']) > plot_df['tfp_pct_change'].abs().quantile(0.99)
print(f"\nOutliers (>99th percentile):")
print(f"EER outliers: {eer_outliers.sum()} observations")
print(f"TFP % change outliers: {tfp_outliers.sum()} observations")


tfp_measures = [
    'tfp_pct_change',          # Original TFP percent change
    'tfp_pct_change_bds',      # BDS-aligned TFP percent change
    'dlog_tfp_bds',            # Log difference of TFP (1-year)
    'dlog_tfp_bds_3y'          # Log difference of TFP (3-year)
]

dynamism_vars = [
    'eer',                          # Entry-to-Exit Ratio
    'firms_percent_created',        # Entry rate
    'firms_percent_destroyed',      # Exit rate
    'net_job_creation_rate'         # Net job creation rate
]

tfp_measures = [var for var in tfp_measures if var in df.columns]
dynamism_vars = [var for var in dynamism_vars if var in df.columns]

print(f"Found {len(tfp_measures)} TFP measures: {', '.join(tfp_measures)}")
print(f"Found {len(dynamism_vars)} dynamism variables: {', '.join(dynamism_vars)}")

fig, axes = plt.subplots(len(tfp_measures), len(dynamism_vars), 
                        figsize=(5*len(dynamism_vars), 4*len(tfp_measures)))

if len(tfp_measures) == 1:
    axes = axes.reshape(1, -1)
if len(dynamism_vars) == 1:
    axes = axes.reshape(-1, 1)

colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray']

for i, tfp_var in enumerate(tfp_measures):
    for j, dyn_var in enumerate(dynamism_vars):
        ax = axes[i, j] if len(tfp_measures) > 1 and len(dynamism_vars) > 1 else axes[i] if len(tfp_measures) > 1 else axes[j] if len(dynamism_vars) > 1 else axes
        
        plot_data = df[[dyn_var, tfp_var]].dropna()
        
        if len(plot_data) > 0:
            # Scatter plot
            ax.scatter(plot_data[dyn_var], plot_data[tfp_var], 
                      alpha=0.4, s=20, color=colors[j % len(colors)])
            
            if len(plot_data) > 10:
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    plot_data[dyn_var], plot_data[tfp_var]
                )
                
                x_line = np.linspace(plot_data[dyn_var].min(), plot_data[dyn_var].max(), 100)
                y_line = slope * x_line + intercept
                ax.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.8)
                
                textstr = f'$R^2$={r_value**2:.3f}\np={p_value:.3f}'
                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, 
                       verticalalignment='top', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
            
            ax.set_xlabel(dyn_var.replace('_', ' ').title(), fontsize=9)
            ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=9)
            
            ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            if dyn_var == 'eer':
                ax.axvline(x=1, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            elif dyn_var == 'net_job_creation_rate':
                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            
            # Title for top row
            if i == 0:
                ax.set_title(dyn_var.replace('_', ' ').title(), fontsize=11, fontweight='bold')
            
            # Y-axis label for first column
            if j == 0:
                ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=10, fontweight='bold')
            
            ax.grid(True, alpha=0.2)
        else:
            ax.text(0.5, 0.5, 'No data', transform=ax.transAxes, 
                   ha='center', va='center')

plt.suptitle('TFP Measures vs Business Dynamism Variables', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Correlation heatmap
print("\n=== Correlation Matrix: All TFP Measures vs Business Dynamism ===")
corr_data = df[tfp_measures + dynamism_vars].corr()
corr_subset = corr_data.loc[tfp_measures, dynamism_vars]

fig2, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_subset, annot=True, fmt='.3f', cmap='RdBu_r', center=0,
            cbar_kws={'label': 'Correlation Coefficient'},
            xticklabels=[var.replace('_', ' ').title() for var in dynamism_vars],
            yticklabels=[var.replace('_', ' ').title() for var in tfp_measures])
plt.title('Correlation Heatmap: TFP Measures vs Business Dynamism Variables', fontsize=14)
plt.tight_layout()
plt.show()

# Detailed comparison of TFP measures
print("\n=== Comparing TFP Measures ===")
fig3, axes3 = plt.subplots(2, 2, figsize=(14, 12))

# 1. Distribution of TFP measures
ax1 = axes3[0, 0]
for tfp_var in tfp_measures:
    data = df[tfp_var].dropna()
    ax1.hist(data, bins=50, alpha=0.5, label=tfp_var.replace('_', ' ').title(), density=True)
ax1.set_xlabel('TFP Growth Rate')
ax1.set_ylabel('Density')
ax1.set_title('Distribution of Different TFP Measures')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. Correlation between TFP measures
ax2 = axes3[0, 1]
if len(tfp_measures) >= 2:
    # Plot first two TFP measures against each other
    tfp1, tfp2 = tfp_measures[0], tfp_measures[1]
    plot_data = df[[tfp1, tfp2]].dropna()
    ax2.scatter(plot_data[tfp1], plot_data[tfp2], alpha=0.5, s=20)
    
    # Add 45-degree line
    lims = [
        np.min([ax2.get_xlim(), ax2.get_ylim()]),
        np.max([ax2.get_xlim(), ax2.get_ylim()]),
    ]
    ax2.plot(lims, lims, 'k-', alpha=0.75, zorder=0, linewidth=1)
    
    # Calculate correlation
    corr = plot_data[tfp1].corr(plot_data[tfp2])
    ax2.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax2.transAxes,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white'))
    
    ax2.set_xlabel(tfp1.replace('_', ' ').title())
    ax2.set_ylabel(tfp2.replace('_', ' ').title())
    ax2.set_title('Comparison of TFP Measures')
    ax2.grid(True, alpha=0.3)

# 3. Time series of average TFP by measure
ax3 = axes3[1, 0]
for tfp_var in tfp_measures:
    annual_avg = df.groupby('year')[tfp_var].mean()
    ax3.plot(annual_avg.index, annual_avg.values, marker='o', label=tfp_var.replace('_', ' ').title())
ax3.set_xlabel('Year')
ax3.set_ylabel('Average TFP Growth')
ax3.set_title('Average TFP Growth Over Time by Measure')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. Strongest relationships summary
ax4 = axes3[1, 1]
# Find strongest correlations
all_corrs = []
for tfp_var in tfp_measures:
    for dyn_var in dynamism_vars:
        valid_data = df[[tfp_var, dyn_var]].dropna()
        if len(valid_data) > 10:
            corr = valid_data[tfp_var].corr(valid_data[dyn_var])
            all_corrs.append({
                'TFP Measure': tfp_var.replace('_', ' ').title(),
                'Dynamism Variable': dyn_var.replace('_', ' ').title(),
                'Correlation': corr
            })

corr_df = pd.DataFrame(all_corrs)
# Get top 10 by absolute correlation
corr_df['Abs_Corr'] = corr_df['Correlation'].abs()
top_corrs = corr_df.nlargest(10, 'Abs_Corr')

# Create horizontal bar plot
y_pos = np.arange(len(top_corrs))
bars = ax4.barh(y_pos, top_corrs['Correlation'])

# Color bars by sign
for bar, corr in zip(bars, top_corrs['Correlation']):
    bar.set_color('darkgreen' if corr > 0 else 'darkred')

# Labels
labels = [f"{row['TFP Measure'][:15]}... vs {row['Dynamism Variable'][:15]}..." 
          for _, row in top_corrs.iterrows()]
ax4.set_yticks(y_pos)
ax4.set_yticklabels(labels, fontsize=8)
ax4.set_xlabel('Correlation Coefficient')
ax4.set_title('Top 10 Strongest Correlations')
ax4.axvline(x=0, color='black', linestyle='-', alpha=0.3)
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Summary statistics table
print("\n=== Summary Statistics for TFP Measures ===")
tfp_stats = []
for tfp_var in tfp_measures:
    data = df[tfp_var].dropna()
    tfp_stats.append({
        'TFP Measure': tfp_var.replace('_', ' ').title(),
        'Mean': data.mean(),
        'Std Dev': data.std(),
        'Min': data.min(),
        'Max': data.max(),
        'Skewness': data.skew(),
        'N': len(data)
    })

tfp_stats_df = pd.DataFrame(tfp_stats)
print(tfp_stats_df.to_string(index=False))

# Regression results summary
print("\n=== Regression Results Summary ===")
print("Significant relationships (p < 0.05):")
significant_results = []

for tfp_var in tfp_measures:
    for dyn_var in dynamism_vars:
        plot_data = df[[dyn_var, tfp_var]].dropna()
        if len(plot_data) > 10:
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                plot_data[dyn_var], plot_data[tfp_var]
            )
            if p_value < 0.05:
                significant_results.append({
                    'TFP': tfp_var.replace('_', ' ').title()[:20],
                    'Dynamism': dyn_var.replace('_', ' ').title()[:20],
                    'Slope': slope,
                    'R²': r_value**2,
                    'p-value': p_value
                })

if significant_results:
    sig_df = pd.DataFrame(significant_results)
    print(sig_df.to_string(index=False))
else:
    print("No significant relationships found at p < 0.05 level")

# Industry-specific analysis for best performing combination
print("\n=== Industry-Specific Analysis (Top Industries) ===")
if all_corrs:
    # Find the combination with highest R²
    best_combo = max(all_corrs, key=lambda x: abs(x['Correlation']))
    
    # Find the actual variable names from our lists
    best_tfp = None
    best_dyn = None
    
    # Match TFP measure
    for tfp_var in tfp_measures:
        if tfp_var.replace('_', ' ').title() == best_combo['TFP Measure']:
            best_tfp = tfp_var
            break
    
    # Match dynamism variable
    for dyn_var in dynamism_vars:
        if dyn_var.replace('_', ' ').title() == best_combo['Dynamism Variable']:
            best_dyn = dyn_var
            break
    
    if best_tfp and best_dyn:
        print(f"Analyzing: {best_combo['TFP Measure']} vs {best_combo['Dynamism Variable']}")
        print(f"Overall correlation: {best_combo['Correlation']:.3f}")
        
        # Get top 5 industries by observation count
        industry_counts = df.groupby('Industry').size().sort_values(ascending=False)
        top_industries = industry_counts.head(5).index
        
        print("\nIndustry-specific correlations:")
        for industry in top_industries:
            ind_data = df[df['Industry'] == industry][[best_tfp, best_dyn]].dropna()
            if len(ind_data) > 10:
                corr = ind_data[best_tfp].corr(ind_data[best_dyn])
                print(f"{industry[:30]:30} Corr: {corr:6.3f}  N: {len(ind_data):4d}")
    else:
        print("Could not identify best combination variables")
else:
    print("No correlations computed")


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats
from scipy.stats.mstats import winsorize

# Create a copy of the dataframe for winsorized analysis
df_winsorized = df.copy()

# Define all TFP measures
tfp_measures = [
    'tfp_pct_change',          # Original TFP percent change
    'tfp_pct_change_bds',      # BDS-aligned TFP percent change
    'dlog_tfp_bds',            # Log difference of TFP (1-year)
    'dlog_tfp_bds_3y'          # Log difference of TFP (3-year)
]

# Define key business dynamism variables to analyze
dynamism_vars = [
    'eer',                          # Entry-to-Exit Ratio
    'firms_percent_created',        # Entry rate
    'firms_percent_destroyed',      # Exit rate
    'net_job_creation_rate'         # Net job creation rate
]

# Filter to only include variables that exist
tfp_measures = [var for var in tfp_measures if var in df.columns]
dynamism_vars = [var for var in dynamism_vars if var in df.columns]

# Winsorize all relevant variables at the 1% level (1st and 99th percentiles)
print("=== Winsorizing Data at 1% Level ===")
print("\nOriginal data ranges:")
for var in tfp_measures + dynamism_vars:
    if var in df_winsorized.columns:
        original_min = df_winsorized[var].min()
        original_max = df_winsorized[var].max()
        print(f"{var:30} Min: {original_min:10.3f} Max: {original_max:10.3f}")

print("\nWinsorized data ranges:")
for var in tfp_measures + dynamism_vars:
    if var in df_winsorized.columns:
        # Get non-null values
        non_null_mask = ~df_winsorized[var].isna()
        non_null_values = df_winsorized.loc[non_null_mask, var]
        
        # Winsorize the non-null values
        winsorized_values = winsorize(non_null_values, limits=[0.01, 0.01])
        
        # Put the winsorized values back
        df_winsorized.loc[non_null_mask, var] = winsorized_values
        
        # Print new ranges
        new_min = df_winsorized[var].min()
        new_max = df_winsorized[var].max()
        print(f"{var:30} Min: {new_min:10.3f} Max: {new_max:10.3f}")

# Create comprehensive scatter plot matrix with winsorized data
fig, axes = plt.subplots(len(tfp_measures), len(dynamism_vars), 
                        figsize=(5*len(dynamism_vars), 4*len(tfp_measures)))

# Handle single row/column cases
if len(tfp_measures) == 1:
    axes = axes.reshape(1, -1)
if len(dynamism_vars) == 1:
    axes = axes.reshape(-1, 1)

# Color scheme
colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown', 'pink', 'gray']

# Plot each combination
for i, tfp_var in enumerate(tfp_measures):
    for j, dyn_var in enumerate(dynamism_vars):
        ax = axes[i, j] if len(tfp_measures) > 1 and len(dynamism_vars) > 1 else axes[i] if len(tfp_measures) > 1 else axes[j] if len(dynamism_vars) > 1 else axes
        
        # Prepare data
        plot_data = df_winsorized[[dyn_var, tfp_var]].dropna()
        
        if len(plot_data) > 0:
            # Scatter plot
            ax.scatter(plot_data[dyn_var], plot_data[tfp_var], 
                      alpha=0.4, s=20, color=colors[j % len(colors)])
            
            # Add regression line if enough data
            if len(plot_data) > 10:
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    plot_data[dyn_var], plot_data[tfp_var]
                )
                
                # Plot regression line
                x_line = np.linspace(plot_data[dyn_var].min(), plot_data[dyn_var].max(), 100)
                y_line = slope * x_line + intercept
                ax.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.8)
                
                # Add stats to plot
                textstr = f'$R^2$={r_value**2:.3f}\np={p_value:.3f}'
                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, 
                       verticalalignment='top', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
            
            # Formatting
            ax.set_xlabel(dyn_var.replace('_', ' ').title(), fontsize=9)
            ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=9)
            
            # Add reference lines
            ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            if dyn_var == 'eer':
                ax.axvline(x=1, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            elif dyn_var == 'net_job_creation_rate':
                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            
            # Title for top row
            if i == 0:
                ax.set_title(dyn_var.replace('_', ' ').title(), fontsize=11, fontweight='bold')
            
            # Y-axis label for first column
            if j == 0:
                ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=10, fontweight='bold')
            
            ax.grid(True, alpha=0.2)
        else:
            ax.text(0.5, 0.5, 'No data', transform=ax.transAxes, 
                   ha='center', va='center')

plt.suptitle('TFP Measures vs Business Dynamism Variables (Winsorized at 1%)', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Correlation heatmap with winsorized data
print("\n=== Correlation Matrix: All TFP Measures vs Business Dynamism (Winsorized) ===")
corr_data = df_winsorized[tfp_measures + dynamism_vars].corr()
corr_subset = corr_data.loc[tfp_measures, dynamism_vars]

fig2, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_subset, annot=True, fmt='.3f', cmap='RdBu_r', center=0,
            cbar_kws={'label': 'Correlation Coefficient'},
            xticklabels=[var.replace('_', ' ').title() for var in dynamism_vars],
            yticklabels=[var.replace('_', ' ').title() for var in tfp_measures])
plt.title('Correlation Heatmap: TFP Measures vs Business Dynamism Variables (Winsorized)', fontsize=14)
plt.tight_layout()
plt.show()

# Comparison of correlations before and after winsorizing
print("\n=== Comparison of Correlations: Original vs Winsorized ===")
comparison_results = []

for tfp_var in tfp_measures:
    for dyn_var in dynamism_vars:
        # Original correlation
        orig_data = df[[tfp_var, dyn_var]].dropna()
        if len(orig_data) > 10:
            orig_corr = orig_data[tfp_var].corr(orig_data[dyn_var])
            
            # Winsorized correlation
            wins_data = df_winsorized[[tfp_var, dyn_var]].dropna()
            wins_corr = wins_data[tfp_var].corr(wins_data[dyn_var])
            
            comparison_results.append({
                'TFP Measure': tfp_var.replace('_', ' ').title()[:20],
                'Dynamism Variable': dyn_var.replace('_', ' ').title()[:20],
                'Original Corr': orig_corr,
                'Winsorized Corr': wins_corr,
                'Change': wins_corr - orig_corr
            })

comparison_df = pd.DataFrame(comparison_results)
print(comparison_df.to_string(index=False))

# Summary statistics table for winsorized data
print("\n=== Summary Statistics for TFP Measures (Winsorized) ===")
tfp_stats = []
for tfp_var in tfp_measures:
    data = df_winsorized[tfp_var].dropna()
    tfp_stats.append({
        'TFP Measure': tfp_var.replace('_', ' ').title(),
        'Mean': data.mean(),
        'Std Dev': data.std(),
        'Min': data.min(),
        'Max': data.max(),
        'Skewness': data.skew(),
        'N': len(data)
    })

tfp_stats_df = pd.DataFrame(tfp_stats)
print(tfp_stats_df.to_string(index=False))

# Regression results summary with winsorized data
print("\n=== Regression Results Summary (Winsorized) ===")
print("Significant relationships (p < 0.05):")
significant_results = []

for tfp_var in tfp_measures:
    for dyn_var in dynamism_vars:
        plot_data = df_winsorized[[dyn_var, tfp_var]].dropna()
        if len(plot_data) > 10:
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                plot_data[dyn_var], plot_data[tfp_var]
            )
            if p_value < 0.05:
                significant_results.append({
                    'TFP': tfp_var.replace('_', ' ').title()[:20],
                    'Dynamism': dyn_var.replace('_', ' ').title()[:20],
                    'Slope': slope,
                    'R²': r_value**2,
                    'p-value': p_value
                })

if significant_results:
    sig_df = pd.DataFrame(significant_results)
    print(sig_df.to_string(index=False))
else:
    print("No significant relationships found at p < 0.05 level")

# Box plots to show the effect of winsorizing
fig3, axes3 = plt.subplots(2, 2, figsize=(14, 10))

# Select key variables to visualize
key_vars = ['eer', 'tfp_pct_change', 'net_job_creation_rate', 'dlog_tfp_bds']
key_vars = [v for v in key_vars if v in df.columns][:4]

for idx, var in enumerate(key_vars):
    ax = axes3[idx // 2, idx % 2]
    
    # Prepare data
    orig_data = df[var].dropna()
    wins_data = df_winsorized[var].dropna()
    
    # Create box plots
    box_data = [orig_data, wins_data]
    positions = [1, 2]
    
    bp = ax.boxplot(box_data, positions=positions, widths=0.6, patch_artist=True,
                    showfliers=True, flierprops=dict(marker='o', markersize=3, alpha=0.5))
    
    # Color the boxes
    colors = ['lightblue', 'lightgreen']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
    
    # Labels and formatting
    ax.set_xticklabels(['Original', 'Winsorized'])
    ax.set_ylabel(var.replace('_', ' ').title())
    ax.set_title(f'Distribution of {var.replace("_", " ").title()}')
    ax.grid(True, alpha=0.3)
    
    # Add text with outlier counts
    n_outliers_orig = len(orig_data[(orig_data < orig_data.quantile(0.01)) | 
                                   (orig_data > orig_data.quantile(0.99))])
    ax.text(0.02, 0.98, f'Outliers removed: {n_outliers_orig}', 
            transform=ax.transAxes, verticalalignment='top', fontsize=9,
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))

plt.suptitle('Effect of Winsorizing on Key Variables', fontsize=16)
plt.tight_layout()
plt.show()

print("\n=== Analysis Complete ===")
print("The winsorized analysis removes extreme values at the 1st and 99th percentiles.")
print("This should provide more robust estimates of the relationships between TFP and business dynamism.")


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy import stats

# Filter for Computer and electronic products industry
industry_name = 'Computer and electronic products'
comp_elec_df = df[df['Industry'] == industry_name].copy()

print(f"=== Analysis for {industry_name} Industry ===")
print(f"Total observations: {len(comp_elec_df)}")
print(f"Year range: {comp_elec_df['year'].min()} - {comp_elec_df['year'].max()}")

# Define variables
tfp_measures = [
    'tfp_pct_change',
    'tfp_pct_change_bds',
    'dlog_tfp_bds',
    'dlog_tfp_bds_3y'
]

dynamism_vars = [
    'eer',
    'firms_percent_created',
    'firms_percent_destroyed',
    'net_job_creation_rate'
]

# Filter to existing variables
tfp_measures = [var for var in tfp_measures if var in comp_elec_df.columns]
dynamism_vars = [var for var in dynamism_vars if var in comp_elec_df.columns]

# Create scatter plot matrix for this industry
fig, axes = plt.subplots(len(tfp_measures), len(dynamism_vars), 
                        figsize=(5*len(dynamism_vars), 4*len(tfp_measures)))

# Handle single row/column cases
if len(tfp_measures) == 1:
    axes = axes.reshape(1, -1)
if len(dynamism_vars) == 1:
    axes = axes.reshape(-1, 1)

# Color scheme
colors = ['darkblue', 'darkgreen', 'darkred', 'purple']

# Plot each combination
for i, tfp_var in enumerate(tfp_measures):
    for j, dyn_var in enumerate(dynamism_vars):
        ax = axes[i, j] if len(tfp_measures) > 1 and len(dynamism_vars) > 1 else axes[i] if len(tfp_measures) > 1 else axes[j] if len(dynamism_vars) > 1 else axes
        
        # Prepare data
        plot_data = comp_elec_df[[dyn_var, tfp_var, 'year']].dropna()
        
        if len(plot_data) > 0:
            # Scatter plot with year coloring
            scatter = ax.scatter(plot_data[dyn_var], plot_data[tfp_var], 
                               c=plot_data['year'], cmap='viridis',
                               alpha=0.7, s=50, edgecolors='black', linewidth=0.5)
            
            # Add regression line if enough data
            if len(plot_data) > 5:
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    plot_data[dyn_var], plot_data[tfp_var]
                )
                
                # Plot regression line
                x_line = np.linspace(plot_data[dyn_var].min(), plot_data[dyn_var].max(), 100)
                y_line = slope * x_line + intercept
                ax.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.8)
                
                # Add stats to plot
                textstr = f'$R^2$={r_value**2:.3f}\np={p_value:.3f}\nslope={slope:.3f}'
                ax.text(0.05, 0.95, textstr, transform=ax.transAxes, 
                       verticalalignment='top', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
            
            # Formatting
            ax.set_xlabel(dyn_var.replace('_', ' ').title(), fontsize=9)
            ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=9)
            
            # Add reference lines
            ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            if dyn_var == 'eer':
                ax.axvline(x=1, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            elif dyn_var == 'net_job_creation_rate':
                ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3, linewidth=0.5)
            
            # Title for top row
            if i == 0:
                ax.set_title(dyn_var.replace('_', ' ').title(), fontsize=11, fontweight='bold')
            
            # Y-axis label for first column
            if j == 0:
                ax.set_ylabel(tfp_var.replace('_', ' ').title(), fontsize=10, fontweight='bold')
            
            ax.grid(True, alpha=0.2)
            
            # Add colorbar to the rightmost plot
            if j == len(dynamism_vars) - 1 and i == 0:
                cbar = plt.colorbar(scatter, ax=ax)
                cbar.set_label('Year', fontsize=8)
        else:
            ax.text(0.5, 0.5, 'No data', transform=ax.transAxes, 
                   ha='center', va='center')

plt.suptitle(f'{industry_name}: TFP Measures vs Business Dynamism Variables', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Time series plots
fig2, axes2 = plt.subplots(2, 2, figsize=(14, 10))

# 1. TFP measures over time
ax1 = axes2[0, 0]
for tfp_var in tfp_measures[:3]:  # Limit to first 3 for clarity
    data = comp_elec_df[['year', tfp_var]].dropna()
    ax1.plot(data['year'], data[tfp_var], marker='o', label=tfp_var.replace('_', ' ').title(), linewidth=2)
ax1.set_xlabel('Year')
ax1.set_ylabel('TFP Growth Rate')
ax1.set_title(f'{industry_name}: TFP Growth Over Time')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)

# 2. Business dynamism measures over time
ax2 = axes2[0, 1]
for dyn_var in dynamism_vars:
    data = comp_elec_df[['year', dyn_var]].dropna()
    if dyn_var == 'eer':
        ax2_twin = ax2.twinx()
        ax2_twin.plot(data['year'], data[dyn_var], marker='s', 
                     label=dyn_var.replace('_', ' ').title(), linewidth=2, color='red')
        ax2_twin.set_ylabel('Entry-to-Exit Ratio', color='red')
        ax2_twin.tick_params(axis='y', labelcolor='red')
    else:
        ax2.plot(data['year'], data[dyn_var], marker='o', 
                label=dyn_var.replace('_', ' ').title(), linewidth=2)
ax2.set_xlabel('Year')
ax2.set_ylabel('Rate (%)')
ax2.set_title(f'{industry_name}: Business Dynamism Over Time')
ax2.legend(loc='upper left')
ax2.grid(True, alpha=0.3)

# 3. Correlation heatmap for this industry
ax3 = axes2[1, 0]
corr_data = comp_elec_df[tfp_measures + dynamism_vars].corr()
corr_subset = corr_data.loc[tfp_measures, dynamism_vars]

sns.heatmap(corr_subset, annot=True, fmt='.3f', cmap='RdBu_r', center=0,
            ax=ax3, cbar_kws={'label': 'Correlation'},
            xticklabels=[var.replace('_', ' ').title()[:15] for var in dynamism_vars],
            yticklabels=[var.replace('_', ' ').title()[:15] for var in tfp_measures])
ax3.set_title(f'{industry_name}: Correlation Matrix')

# 4. Summary statistics
ax4 = axes2[1, 1]
ax4.axis('off')

# Create summary text
summary_text = f"Summary Statistics for {industry_name}\n"
summary_text += "="*50 + "\n\n"

# TFP statistics
summary_text += "TFP Measures (Mean ± Std):\n"
for tfp_var in tfp_measures:
    data = comp_elec_df[tfp_var].dropna()
    if len(data) > 0:
        summary_text += f"{tfp_var.replace('_', ' ').title()[:25]:25} {data.mean():6.2f} ± {data.std():6.2f}\n"

summary_text += "\nBusiness Dynamism (Mean ± Std):\n"
for dyn_var in dynamism_vars:
    data = comp_elec_df[dyn_var].dropna()
    if len(data) > 0:
        summary_text += f"{dyn_var.replace('_', ' ').title()[:25]:25} {data.mean():6.2f} ± {data.std():6.2f}\n"

# Add correlation summary
summary_text += "\nStrongest Correlations:\n"
all_corrs = []
for tfp_var in tfp_measures:
    for dyn_var in dynamism_vars:
        valid_data = comp_elec_df[[tfp_var, dyn_var]].dropna()
        if len(valid_data) > 5:
            corr = valid_data[tfp_var].corr(valid_data[dyn_var])
            all_corrs.append((tfp_var, dyn_var, corr))

# Sort by absolute correlation
all_corrs.sort(key=lambda x: abs(x[2]), reverse=True)
for tfp_var, dyn_var, corr in all_corrs[:5]:
    summary_text += f"{tfp_var[:15]} vs {dyn_var[:15]}: {corr:6.3f}\n"

ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, 
         verticalalignment='top', fontsize=10, family='monospace')

plt.tight_layout()
plt.show()

# Detailed regression analysis
print("\n=== Detailed Regression Results ===")
for tfp_var in tfp_measures:
    print(f"\nDependent Variable: {tfp_var}")
    print("-" * 60)
    for dyn_var in dynamism_vars:
        plot_data = comp_elec_df[[dyn_var, tfp_var]].dropna()
        if len(plot_data) > 5:
            slope, intercept, r_value, p_value, std_err = stats.linregress(
                plot_data[dyn_var], plot_data[tfp_var]
            )
            print(f"{dyn_var:30} Slope: {slope:8.4f}  R²: {r_value**2:6.3f}  p-value: {p_value:6.3f}  N: {len(plot_data):3d}")

# Compare with overall sample
print("\n=== Comparison with Full Sample ===")
print(f"Industry: {industry_name}")
print(f"Industry observations: {len(comp_elec_df)}")
print(f"Total sample observations: {len(df)}")
print(f"Industry share: {len(comp_elec_df)/len(df)*100:.1f}%")

# Check for any notable patterns
print("\n=== Notable Patterns ===")
# Check if TFP is consistently positive/negative
avg_tfp = comp_elec_df['tfp_pct_change'].mean()
if avg_tfp > 1:
    print(f"- High average TFP growth: {avg_tfp:.2f}% (well above sample average)")
elif avg_tfp < -1:
    print(f"- Low average TFP growth: {avg_tfp:.2f}% (well below sample average)")

# Check business dynamism trends
recent_years = comp_elec_df[comp_elec_df['year'] >= comp_elec_df['year'].max() - 5]
if len(recent_years) > 0:
    recent_njcr = recent_years['net_job_creation_rate'].mean()
    overall_njcr = comp_elec_df['net_job_creation_rate'].mean()
    if abs(recent_njcr - overall_njcr) > 2:
        trend = "increasing" if recent_njcr > overall_njcr else "decreasing"
        print(f"- Net job creation rate {trend} in recent years")

# Export data for further analysis
comp_elec_summary = comp_elec_df[['year'] + tfp_measures + dynamism_vars].describe()
print("\n=== Descriptive Statistics ===")
print(comp_elec_summary)





# ------------------------------------------------------------------
# 3.2  Year OR industry fixed effects, cluster by industry
# ------------------------------------------------------------------
# tfp_ann_pct_lead, tfp_pct_change_lead, tfp_diff3, tfp_ann_pct, tfp_log, tfp_index_2017
# eer, pct_high_growth_emp, reallocation_rate

# 1. First, check for missing values in the data
missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

# 2. Drop missing values explicitly to ensure consistency
df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

# 3. Run the regression with the clean data
mod2 = smf.ols(
    'tfp_ann_pct_lead ~ pct_high_growth_emp + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


# ------------------------------------------------------------------
# 3.2  Year OR industry fixed effects, cluster by industry
# ------------------------------------------------------------------
# tfp_ann_pct_lead, tfp_pct_change_lead, tfp_diff3, tfp_ann_pct, tfp_log, tfp_index_2017
# eer, pct_high_growth_emp, reallocation_rate

# 1. First, check for missing values in the data
missing_values = df[['tfp_pct_change_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

# 2. Drop missing values explicitly to ensure consistency
df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

# 3. Run the regression with the clean data
mod2 = smf.ols(
    'tfp_pct_change_lead ~ pct_high_growth_emp + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


# ------------------------------------------------------------------
# 3.2  Year OR industry fixed effects, cluster by industry
# ------------------------------------------------------------------
# tfp_ann_pct_lead, tfp_pct_change_lead, tfp_diff3, tfp_ann_pct, tfp_log, tfp_index_2017
# eer, pct_high_growth_emp, reallocation_rate

# 1. First, check for missing values in the data
missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

# 2. Drop missing values explicitly to ensure consistency
df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

# 3. Run the regression with the clean data
mod2 = smf.ols(
    'tfp_ann_pct_lead ~ reallocation_rate + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


# ------------------------------------------------------------------
# 3.2  Year OR industry fixed effects, cluster by industry
# ------------------------------------------------------------------
# tfp_ann_pct_lead, tfp_pct_change_lead, tfp_diff3, tfp_ann_pct, tfp_log, tfp_index_2017
# eer, pct_high_growth_emp, reallocation_rate

# 1. First, check for missing values in the data
missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

# 2. Drop missing values explicitly to ensure consistency
df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

# 3. Run the regression with the clean data
mod2 = smf.ols(
    'tfp_ann_pct_lead ~ eer + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


import pandas as pd
import statsmodels.formula.api as smf
from itertools import product

# Make sure the panel is sorted
df = df.sort_values(['Industry', 'year'])

# Define dependent and independent variables
dependent_vars = [
    'tfp_ann_pct_lead', 
    'tfp_pct_change_lead', 
    'tfp_diff3', 
    'tfp_ann_pct', 
    'tfp_log', 
    'tfp_index_2017'
]

independent_vars = [
    'eer', 
    'pct_high_growth_emp', 
    'reallocation_rate'
]

# Create a dictionary to store results
results_fe = {}

print("="*80)
print("OLS REGRESSION RESULTS WITH INDUSTRY FIXED EFFECTS")
print("="*80)

# Loop through all combinations of dependent and independent variables
for dep_var in dependent_vars:
    for indep_var in independent_vars:
        
        # Create the formula with industry fixed effects
        formula = f'{dep_var} ~ {indep_var} + C(Industry)'
        
        try:
            # Run the regression with industry fixed effects
            model = smf.ols(formula, data=df).fit(cov_type='HC1')
            
            # Store results
            key = f'{dep_var}_vs_{indep_var}_FE'
            results_fe[key] = model
            
            # Print summary information
            print(f"\n{'-'*60}")
            print(f"MODEL: {dep_var} ~ {indep_var} + Industry FE")
            print(f"{'-'*60}")
            print(f"R-squared: {model.rsquared:.4f}")
            print(f"Adj. R-squared: {model.rsquared_adj:.4f}")
            print(f"F-statistic: {model.fvalue:.4f}")
            print(f"Prob (F-statistic): {model.f_pvalue:.4f}")
            print(f"N observations: {model.nobs:.0f}")
            
            # Print coefficient information for the main variable of interest
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]  # Robust standard errors
            t_stat = model.tvalues[indep_var]
            p_value = model.pvalues[indep_var]
            
            print(f"\nCoefficient on {indep_var}:")
            print(f"  Estimate: {coef:.6f}")
            print(f"  Std Error: {se:.6f}")
            print(f"  t-statistic: {t_stat:.4f}")
            print(f"  p-value: {p_value:.4f}")
            print(f"  Significance: {'***' if p_value < 0.01 else '**' if p_value < 0.05 else '*' if p_value < 0.10 else ''}")
            
            # Count number of industries (fixed effects)
            industry_params = [param for param in model.params.index if 'C(Industry)' in param]
            print(f"  Number of Industry FE: {len(industry_params)}")
            
        except Exception as e:
            print(f"\nERROR with {dep_var} ~ {indep_var} + Industry FE: {str(e)}")
            continue

# Create a summary table of all results
print("\n" + "="*80)
print("SUMMARY TABLE OF ALL REGRESSIONS WITH INDUSTRY FIXED EFFECTS")
print("="*80)

summary_data_fe = []
for dep_var in dependent_vars:
    for indep_var in independent_vars:
        key = f'{dep_var}_vs_{indep_var}_FE'
        if key in results_fe:
            model = results_fe[key]
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]
            p_val = model.pvalues[indep_var]
            r2 = model.rsquared
            adj_r2 = model.rsquared_adj
            n_obs = model.nobs
            
            # Count industry fixed effects
            industry_params = [param for param in model.params.index if 'C(Industry)' in param]
            n_industries = len(industry_params)
            
            summary_data_fe.append({
                'Dependent Variable': dep_var,
                'Independent Variable': indep_var,
                'Coefficient': f"{coef:.6f}",
                'Std Error': f"{se:.6f}",
                'p-value': f"{p_val:.4f}",
                'R-squared': f"{r2:.4f}",
                'Adj R-squared': f"{adj_r2:.4f}",
                'N': f"{n_obs:.0f}",
                'Industry FE': f"{n_industries}",
                'Significance': '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*' if p_val < 0.10 else ''
            })

# Convert to DataFrame for nice display
summary_df_fe = pd.DataFrame(summary_data_fe)
print(summary_df_fe.to_string(index=False))

# Optional: Save results to CSV
summary_df_fe.to_csv('/Users/danielseymour/Developer/EC334-Summative/processed_data/ols_industry_fe_results.csv', index=False)
print(f"\nResults saved to: /Users/danielseymour/Developer/EC334-Summative/processed_data/ols_industry_fe_results.csv")

# If you want to access individual model results later:
print(f"\nTo access individual models, use: results_fe['dependent_var_vs_independent_var_FE']")
print(f"Example: results_fe['tfp_ann_pct_lead_vs_eer_FE'].summary()")

# Example of accessing a specific model's full summary
print(f"\n" + "="*80)
print("EXAMPLE: Full summary for tfp_ann_pct_lead ~ eer + Industry FE")
print("="*80)
if 'tfp_ann_pct_lead_vs_eer_FE' in results_fe:
    print(results_fe['tfp_ann_pct_lead_vs_eer_FE'].summary())

# Additional: Show which industries are included as fixed effects
print(f"\n" + "="*80)
print("INDUSTRY FIXED EFFECTS INCLUDED")
print("="*80)
if 'tfp_ann_pct_lead_vs_eer_FE' in results_fe:
    model_example = results_fe['tfp_ann_pct_lead_vs_eer_FE']
    industry_effects = [param for param in model_example.params.index if 'C(Industry)' in param]
    print(f"Total number of industry fixed effects: {len(industry_effects)}")
    
    # Show a few examples of the industry coefficients
    print(f"\nExample industry fixed effect coefficients:")
    for i, effect in enumerate(industry_effects[:5]):  # Show first 5
        coef = model_example.params[effect]
        print(f"  {effect}: {coef:.6f}")
    if len(industry_effects) > 5:
        print(f"  ... and {len(industry_effects) - 5} more industry fixed effects")

# Summary comparison info
print(f"\n" + "="*80)
print("NOTE: COMPARISON WITH SIMPLE OLS")
print("="*80)
print("The industry fixed effects control for time-invariant industry characteristics.")
print("Compare R-squared values with the simple OLS results to see the improvement.")
print("The coefficient on your main variables now represents within-industry variation.")





df.columns


# Explore autocorrelation in the variables
import statsmodels.api as sm

# 1. Pick the regression whose residuals you want to test.
#    Say this is your year‐FE model mod2:
resid = mod2.resid
out = df_clean.copy()
out['resid'] = resid

# 2. For each industry, compute the lag-1 residual and the first difference:
out = out.sort_values(['Industry','year'])
out['resid_lag1'] = out.groupby('Industry')['resid'].shift(1)
out['d_resid']    = out['resid'] - out['resid_lag1']

# 3. Drop the missing first year of each panel:
test_df = out.dropna(subset=['resid_lag1','d_resid'])

# 4. Run the auxiliary regression Δresid_it ~ resid_{i,t–1}:
aux = sm.OLS(test_df['d_resid'], sm.add_constant(test_df['resid_lag1'])).fit()

print(aux.summary())



# 1. Create the lagged dependent variable (same as before)
df_clean['tfp_ann_pct_lead_lag1'] = (
    df_clean
    .sort_values(['Industry','year'])
    .groupby('Industry')['tfp_ann_pct_lead']
    .shift(1)
)

# 2. Drop any rows with missing lag
df_dyn = df_clean.dropna(subset=['tfp_ann_pct_lead_lag1', 'tfp_ann_pct_lead'])

# 3. Run AR(1) model - just the lagged dependent variable with year fixed effects
mod_ar1 = smf.ols(
    'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + C(year)',
    data=df_dyn
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_dyn['NAICS']}
)

print("AR(1) Model - TFP with its own lag:")
print(mod_ar1.summary())


# 1. Create the lagged dependent variable
#    Shift tfp_ann_pct_lead _down_ by one year within each industry
# eer, pct_high_growth_emp, reallocation_rate

df_clean['tfp_ann_pct_lead_lag1'] = (
    df_clean
    .sort_values(['Industry','year'])
    .groupby('Industry')['tfp_ann_pct_lead']
    .shift(1)
)

# 2. Drop any rows that now have a missing lag
df_dyn = df_clean.dropna(subset=['tfp_ann_pct_lead_lag1', 'tfp_ann_pct_lead'])

# 3. Re‐run your fixed‐effect regression, clustering by industry (or year)
mod_dyn = smf.ols(
    'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + eer + C(year)',
    data=df_dyn
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_dyn['NAICS']}    # cluster by industry
)

print(mod_dyn.summary())


# 1. Create lags for the independent variables
dyn_vars = ['eer', 'pct_high_growth_emp', 'reallocation_rate']  # adjust to your actual variables

for var in dyn_vars:
    df_clean[f'{var}_lag1'] = (
        df_clean
        .sort_values(['Industry','year'])
        .groupby('Industry')[var]
        .shift(1)
    )

# 2. Drop rows with any missing values in the lagged variables
lag_vars = ['tfp_ann_pct_lead_lag1'] + [f'{var}_lag1' for var in dyn_vars]
df_dyn = df_clean.dropna(subset=lag_vars + ['tfp_ann_pct_lead'])



# Bivariate regressions with lagged independent variables
for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ TFP_lag + {var}_lag")
    print(f"{'='*60}")
    
    # Make sure we have the necessary data
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', 'tfp_ann_pct_lead_lag1', f'{var}_lag1'])
    
    # Run regression with just one lagged independent variable
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + {var}_lag1 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    print(mod_bivariate.summary())


# Bivariate regressions with lagged independent variables
for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ TFP_lag + {var}_lag")
    print(f"{'='*60}")
    
    # Make sure we have the necessary data
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', 'tfp_ann_pct_lead_lag1', f'{var}_lag1'])
    
    # Run regression with just one lagged independent variable
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ {var}_lag1 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    print(mod_bivariate.summary())


# Focus on just the variables of interest
for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ {var}_lag1 + {var}_lag2")
    print(f"{'='*60}")
    
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', f'{var}_lag1', f'{var}_lag2'])
    
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ {var}_lag1 + {var}_lag2 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    # Print only key statistics and the coefficients of interest
    print(f"R-squared: {mod_bivariate.rsquared:.4f}")
    print(f"N: {mod_bivariate.nobs}")
    print(f"\nCoefficients of interest:")
    print(f"{var}_lag1: {mod_bivariate.params[f'{var}_lag1']:.4f} (p={mod_bivariate.pvalues[f'{var}_lag1']:.4f})")
    print(f"{var}_lag2: {mod_bivariate.params[f'{var}_lag2']:.4f} (p={mod_bivariate.pvalues[f'{var}_lag2']:.4f})")





df.head()


df.columns


import numpy as np
import pandas as pd
import statsmodels.api as sm

# 1) Choose your windows:
early_years = (1988, 1992)   # inclusive, before COVID
late_years  = (2015, 2019)

# 2) Select the variables you care about:
dyn_vars = ['eer','estabs_entry_rate', 'reallocation_rate', 'pct_high_growth_emp']
prod_var = 'tfp_index_2017'

# 3) Compute the within‐window industry means:
early = (df['year'].between(*early_years))
late  = (df['year'].between(*late_years))

common = dyn_vars + [prod_var, 'weight']

early_means = (
    df[early]
      .groupby('NAICS')[ common ]
      .mean()
      .rename(columns=lambda c: f"{c}_early")
)

late_means = (
    df[late]
      .groupby('NAICS')[ common ]
      .mean()
      .rename(columns=lambda c: f"{c}_late")
)

wide = early_means.join(late_means, how='inner')


# 5) Compute long‐run log‐differences:
for v in dyn_vars:
    wide[f"long_run_diff_log_{v}"] = np.log(wide[f"{v}_late"]) - np.log(wide[f"{v}_early"])


wide["long_run_diff_log_TFP"] = np.log(wide["tfp_index_2017_late"]) - np.log(wide["tfp_index_2017_early"])


wide.head()


df.columns


wide.columns


from statsmodels.stats.outliers_influence import variance_inflation_factor
vifs = pd.DataFrame({
    'var': X.columns,
    'VIF': [variance_inflation_factor(X.values, i) 
            for i in range(X.shape[1])]
})
print(vifs)


# Now you have one row per industry, with Δlog_TFP and Δlog_dynamism_i for each margin.
# 6) Cross‐sectional regression:
X = wide[[f"long_run_diff_log_{v}" for v in dyn_vars]]
X = sm.add_constant(X)
y = wide["long_run_diff_log_TFP"]

# Clustered standard errors by industry (NAICS)
model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': wide.index})
print(model.summary())



# Bivariate regressions - one for each dynamism variable
for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate regression: Δlog TFP ~ Δlog {var}")
    print(f"{'='*60}")
    
    # Create X with just one variable (plus constant)
    X_single = wide[[f"long_run_diff_log_{var}"]]
    X_single = sm.add_constant(X_single)
    y = wide["long_run_diff_log_TFP"]
    
    # Run regression with clustered standard errors
    model_single = sm.OLS(y, X_single).fit(cov_type='cluster', cov_kwds={'groups': wide.index})
    print(model_single.summary())


wide.columns


import numpy as np
import statsmodels.api as sm

# 1) Construct log‐size and log‐initial‐TFP controls
wide['log_weight_early']   = np.log(wide['weight_early'])
wide['log_tfp0']           = np.log(wide['tfp_index_2017_early'])

# 2) Define your full X‐matrix:
X = wide[[
    'long_run_diff_log_eer',
    'long_run_diff_log_reallocation_rate',
    'long_run_diff_log_pct_high_growth_emp',
    'log_weight_early',
]]

X = sm.add_constant(X)

# 3) Dependent variable:
y = wide['long_run_diff_log_TFP']

# 4) Estimate with robust SEs:
model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# 1) Build the same X‐matrix you used in your regression (just the regressors)
X = wide[[
    'long_run_diff_log_eer',
    'long_run_diff_log_reallocation_rate',
    'long_run_diff_log_pct_high_growth_emp',
    'log_weight_early'
]].copy()

# 2) Add a constant column (required for VIF calculation)
X['const'] = 1.0

# 3) Compute VIF for each column
vif_data = pd.DataFrame({
    'variable': X.columns,
    'VIF': [
        variance_inflation_factor(X.values, i)
        for i in range(X.shape[1])
    ]
})

print(vif_data)


# Individual 

import statsmodels.api as sm

# Suppose weight_early is the industry’s output share in the early window
w = wide['weight_early']

# 1) Build X & y as before, but omit the size control if you like,
#    since weighting already accounts for scale.
X = wide[['long_run_diff_log_eer']]      # or add other margins here
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

# 2) Fit a WLS model
wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


import numpy as np
import statsmodels.api as sm

# 1) Construct log‐size and log‐initial‐TFP controls
wide['log_weight_early']   = np.log(wide['weight_early'])

# 2) Define your full X‐matrix:
X = wide[[
    'long_run_diff_log_eer',
    'log_weight_early',
]]

X = sm.add_constant(X)

# 3) Dependent variable:
y = wide['long_run_diff_log_TFP']

# 4) Estimate with robust SEs:
model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


wide.columns


import numpy as np
import statsmodels.api as sm

# 1) Construct log‐size and log‐initial‐TFP controls
wide['log_weight_early']   = np.log(wide['weight_early'])

# 2) Define your full X‐matrix:
X = wide[[
    'long_run_diff_log_pct_high_growth_emp',
]]

X = sm.add_constant(X)

# 3) Dependent variable:
y = wide['long_run_diff_log_TFP']

# 4) Estimate with robust SEs:
model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


# Individual 

import statsmodels.api as sm

# Suppose weight_early is the industry’s output share in the early window
w = wide['weight_early']

# 1) Build X & y as before, but omit the size control if you like,
#    since weighting already accounts for scale.
X = wide[['long_run_diff_log_pct_high_growth_emp']]      # or add other margins here
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

# 2) Fit a WLS model
wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


import numpy as np
import statsmodels.api as sm

# 1) Construct log‐size and log‐initial‐TFP controls
wide['log_weight_early']   = np.log(wide['weight_early'])

# 2) Define your full X‐matrix:
X = wide[[
    'long_run_diff_log_reallocation_rate',
]]

X = sm.add_constant(X)

# 3) Dependent variable:
y = wide['long_run_diff_log_TFP']

# 4) Estimate with robust SEs:
model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


# Individual 

import statsmodels.api as sm

# Suppose weight_early is the industry’s output share in the early window
w = wide['weight_early']

# 1) Build X & y as before, but omit the size control if you like,
#    since weighting already accounts for scale.
X = wide[['long_run_diff_log_reallocation_rate']]      # or add other margins here
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

# 2) Fit a WLS model
wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


import numpy as np
import statsmodels.api as sm

wide['log_weight_early']   = np.log(wide['weight_early'])
wide['log_tfp0']           = np.log(wide['tfp_index_2017_early'])

X = wide[[
    'long_run_diff_log_estabs_entry_rate'
]]

X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())

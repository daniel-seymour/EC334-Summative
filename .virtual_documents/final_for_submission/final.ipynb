








import pandas as pd
import os


filepath = '../raw_data/productivity_data/total-factor-productivity-manufacturing-and-transportation-detailed-industries.xlsx'
manufacturing_tfp = pd.read_excel(filepath)


manufacturing_tfp.columns = manufacturing_tfp.iloc[1]
manufacturing_tfp = manufacturing_tfp.drop(manufacturing_tfp.index[:2])
manufacturing_tfp = manufacturing_tfp.reset_index(drop=True)


manufacturing_4digit = manufacturing_tfp[
    manufacturing_tfp['NAICS']
      .astype(str)
      .str.len()
    == 4
]


long_df = manufacturing_4digit.melt(
    id_vars=['NAICS', 'IndustryTitle', 'Basis', 'Measure', 'Units'], 
    var_name='year', 
    value_name='value'
)


tfp_long = long_df[long_df['Measure'] == 'Total factor productivity'].copy()


tfp_pivoted = tfp_long.pivot(
    index=['NAICS', 'IndustryTitle', 'Basis', 'year'],
    columns='Units',
    values='value'
).reset_index()


tfp_pivoted.columns.name = None
tfp_pivoted = tfp_pivoted.rename(columns={
    'Index (2017=100)': 'tfp_index_2017',
    '% Change from previous year': 'tfp_pct_change'
})


out_dir = "../processed_data"
os.makedirs(out_dir, exist_ok=True)
out_path = os.path.join(out_dir, "tfp_four_digit_pivoted.csv")
tfp_pivoted.to_csv(out_path, index=False)
print(f"Saved pivoted TFP data to {out_path}")





import pandas as pd
filepath = '../raw_data/dynamism_data/bds2022_gr_vcn4.csv'
df = pd.read_csv(filepath, encoding='latin1')


print("Number of NaN values in fempgr_gr:", df['fempgr_gr'].isna().sum())
print("Total rows:", len(df))

print("Data type of fempgr_gr:", df['fempgr_gr'].dtype)

print("\nFirst 10 values of fempgr_gr:")
print(df['fempgr_gr'].head(10))

print("\nAny non-NaN values?", df['fempgr_gr'].notna().any())


df['emp'] = pd.to_numeric(df['emp'], errors='coerce')

df['is_high_growth'] = df['fempgr_gr'].isin(['h) [0.8 to 2)', 'i) 2']).astype(int)

df['emp_high_growth'] = df['emp'] * df['is_high_growth']

hgfs_by_industry = df.groupby(['year', 'vcnaics4']).agg({
    'emp_high_growth': 'sum',
    'emp': 'sum'
}).reset_index()

hgfs_by_industry['pct_high_growth_emp'] = (hgfs_by_industry['emp_high_growth'] / hgfs_by_industry['emp']) * 100

hgfs_by_industry = hgfs_by_industry[['year', 'vcnaics4', 'pct_high_growth_emp']]

hgfs_by_industry = hgfs_by_industry.sort_values(['year', 'vcnaics4']).reset_index(drop=True)

output_path = '../processed_data/hgfs_by_industry_naics4.csv'
hgfs_by_industry.to_csv(output_path, index=False)
print(f"\nCSV file saved to: {output_path}")
print(f"File contains {len(hgfs_by_industry)} rows")





import pandas as pd
filepath = '../raw_data/dynamism_data/bds2022_gr_vcn3.csv'
df = pd.read_csv(filepath, encoding='latin1', low_memory=False)


print("Number of NaN values in fempgr_gr:", df['fempgr_gr'].isna().sum())
print("Total rows:", len(df))

print("Data type of fempgr_gr:", df['fempgr_gr'].dtype)
print("\nFirst 10 values of fempgr_gr:")
print(df['fempgr_gr'].head(10))

print("\nAny non-NaN values?", df['fempgr_gr'].notna().any())


df['fempgr_gr'].unique()


df['emp'] = pd.to_numeric(df['emp'], errors='coerce')
df['is_high_growth'] = df['fempgr_gr'].isin(['h) [0.8 to 2)', 'i) 2']).astype(int)
df['emp_high_growth'] = df['emp'] * df['is_high_growth']

hgfs_by_industry = df.groupby(['year', 'vcnaics3']).agg({
    'emp_high_growth': 'sum',
    'emp': 'sum'
}).reset_index()

hgfs_by_industry['pct_high_growth_emp'] = (hgfs_by_industry['emp_high_growth'] / hgfs_by_industry['emp']) * 100
hgfs_by_industry = hgfs_by_industry[['year', 'vcnaics3', 'pct_high_growth_emp']]
hgfs_by_industry = hgfs_by_industry.sort_values(['year', 'vcnaics3']).reset_index(drop=True)

print(hgfs_by_industry.head(10))
print(f"\nDataframe shape: {hgfs_by_industry.shape}")


output_path = '../processed_data/hgfs_by_industry.csv'
hgfs_by_industry.to_csv(output_path, index=False)

print(f"CSV file saved to: {output_path}")





import pandas as pd

sectoral_tfp_df_file_path = '../raw_data/productivity_data/major-industry-total-factor-productivity-klems.xlsx'
sectoral_tfp_df = pd.read_excel(sectoral_tfp_df_file_path, header=2)

bds_3_df_file_path = '../raw_data/dynamism_data/bds2022_vcn3.csv'
bds_3_df = pd.read_csv(bds_3_df_file_path)


sectoral_tfp_df['NAICS'].unique()


sectoral_tfp_df['NAICS'] = sectoral_tfp_df['NAICS'].replace({
    'MN': '31-33',  
    'DM': '33',     
    'ND': '31'      
})


sectoral_tfp_df['NAICS'].unique()


three_digit_sectoral_df = sectoral_tfp_df[sectoral_tfp_df['NAICS'].str.len() == 3].copy()

print(sorted(three_digit_sectoral_df['NAICS'].unique()))


long_df = three_digit_sectoral_df.melt(
    id_vars=['NAICS', 'Industry', 'Basis', 'Measure', 'Units'], 
    var_name='year', 
    value_name='value' 
)


tfp_long = long_df[long_df['Measure'] == 'Total factor productivity'].copy()
output_long = long_df[long_df['Measure'] == 'Sectoral output'].copy()


output_pivot = output_long.pivot_table(
    values='value', 
    index='year', 
    columns='NAICS', 
    aggfunc='sum',
    fill_value=0
)

weights = (
    output_pivot.apply(lambda row: row / (row.sum() + 1e-10), axis=1)
    .stack()
    .reset_index()
    .rename(columns={0: 'weight'})
)

# Merge weights with TFP data
tfp_with_weights = tfp_long.merge(
    weights,
    on=['year', 'NAICS'],
    how='left'
).fillna({'weight': 0})


tfp_units = tfp_with_weights['Units'].value_counts()
print("Units used for Total factor productivity:\n", tfp_units)

tfp_pivoted = tfp_with_weights.pivot(
    index=['NAICS', 'Industry', 'Basis', 'year', 'weight'],
    columns='Units',
    values='value'
).reset_index()

tfp_pivoted.columns.name = None

tfp_pivoted = tfp_pivoted.rename(columns={
    'Index (2017=100)': 'tfp_index_2017',
    '% Change from previous year': 'tfp_pct_change'
})


bds_3_df = bds_3_df.rename(columns={'vcnaics3': 'NAICS'})


bds_3_df['year'] = bds_3_df['year'].astype(int)
bds_87_on = bds_3_df[bds_3_df['year'] >= 1987]


print("bds_87_on NAICS unique values:", bds_87_on['NAICS'].unique()[:5])
print("tfp_pivoted NAICS unique values:", tfp_pivoted['NAICS'].unique()[:5])


print("bds_87_on columns:", bds_87_on.columns.tolist())
print("tfp_pivoted columns:", tfp_pivoted.columns.tolist())


bds_87_on = bds_87_on.copy()
bds_87_on['NAICS'] = bds_87_on['NAICS'].astype(str)
tfp_pivoted['NAICS'] = tfp_pivoted['NAICS'].astype(str)


tfp_pivoted['year'] = tfp_pivoted['year'].astype(int)

# 6) Now merge
merged = pd.merge(
    bds_87_on,
    tfp_pivoted, 
    on=['NAICS','year'],   
    how='inner'
)


object_columns = [
    'estabs_entry', 'estabs_entry_rate', 'estabs_exit', 'estabs_exit_rate',
    'job_creation_births', 'job_creation_rate_births', 'job_destruction_deaths',
    'job_destruction_rate_deaths', 'firmdeath_estabs',
    'tfp_pct_change', 'tfp_index_2017'
]

for col in object_columns:
    merged[col] = pd.to_numeric(merged[col], errors='coerce')


pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


merged['firmdeath_firms'] = pd.to_numeric(merged['firmdeath_firms'], errors='coerce')
merged['firms'] = pd.to_numeric(merged['firms'], errors='coerce')
merged['firms_percent_destroyed'] = (merged['firmdeath_firms'] / merged['firms']) * 100


output_path = '../processed_data/three_digit_NAICS_bds_tfp.csv'
merged.to_csv(output_path, index=False)
print(f"Data saved to {output_path}")





import pandas as pd
import numpy as np
from linearmodels.panel import PanelOLS
import statsmodels.api as sm
from scipy import stats  


filepath = '../raw_data/productivity_data/total_factor_productivity.csv'
tfp_disp_df = pd.read_csv(filepath, encoding='latin1')


filepath = '../processed_data/tfp_four_digit_pivoted.csv'
manufacturing_tfp = pd.read_csv(filepath)


filepath = '../raw_data/dynamism_data/bds2022_vcn4.csv'
bds2022_vcn4 = pd.read_csv(filepath, encoding='latin1')


tfp_disp_df['naics4'] = tfp_disp_df['naics4'].astype(str)
bds2022_vcn4['vcnaics4'] = bds2022_vcn4['vcnaics4'].astype(str)

temp_df = pd.merge(
	tfp_disp_df, 
	bds2022_vcn4, 
	how='inner', 
	left_on=['naics4', 'year'], 
	right_on=['vcnaics4', 'year']
)


temp_df['naics4'] = pd.to_numeric(temp_df['naics4'], errors='raise').astype('int64')
manufacturing_tfp['NAICS'] = pd.to_numeric(manufacturing_tfp['NAICS'], errors='raise').astype('int64')

temp_df['year'] = temp_df['year'].astype(int)
manufacturing_tfp['year'] = pd.to_numeric(manufacturing_tfp['year'], errors='coerce').astype('Int64')
manufacturing_tfp = manufacturing_tfp.dropna(subset=['year'])
manufacturing_tfp['year'] = manufacturing_tfp['year'].astype(int)

merged_df = pd.merge(
    temp_df,
    manufacturing_tfp,
    how='inner',
    left_on = ['naics4', 'year'],
    right_on= ['NAICS',  'year']
)


merged_df['tfp_pct_change'] = pd.to_numeric(
    merged_df['tfp_pct_change'],
    errors='coerce'
)
merged_df['tfp_index_2017'] = pd.to_numeric(
    merged_df['tfp_index_2017'],
    errors='coerce')


columns_to_convert = [
    # Dispersion measures
    'd9990', 'd1001', 'd9990*', 'd1001*',
    
    # Entry/exit counts
    'estabs_entry', 'estabs_exit',
    'firmdeath_firms', 'firmdeath_estabs', 'firmdeath_emp',
    
    # Entry/exit rates  
    'estabs_entry_rate', 'estabs_exit_rate',
    
    # Job flow counts
    'job_creation_births', 'job_creation_continuers',
    'job_destruction_deaths', 'job_destruction_continuers',
    
    # Job flow rates
    'job_creation_rate_births', 'job_destruction_rate_deaths'
]

print("Converting object columns to numeric...")
for col in columns_to_convert:
    if col in merged_df.columns and merged_df[col].dtype == 'object':
        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')
        nan_count = merged_df[col].isna().sum()
        if nan_count > 0:
            print(f"  {col}: converted ({nan_count} NaN values created)")
        else:
            print(f"  {col}: converted successfully")


agg_dict = {
   # DISPERSION MEASURES - AVERAGE within 2-year window
   'sd': 'mean',
   'd7525': 'mean',
   'd9010': 'mean',
   'd9990': 'mean',
   'd1001': 'mean',
   'sd*': 'mean',
   'd7525*': 'mean',
   'd9010*': 'mean',
   'd9990*': 'mean',
   'd1001*': 'mean',

   # TFP metrics
   'tfp_pct_change': 'mean',     # average % change over the 2-year window
   'tfp_index_2017': 'last',     # index level at the end of the window

   # STOCK VARIABLES - LAST value (end of period snapshot)
   'firms': 'last',
   'estabs': 'last',
   'emp': 'last',
   'denom': 'last',

   # ENTRY/EXIT COUNTS - SUM across 2 years
   'estabs_entry': 'sum',
   'estabs_exit': 'sum',
   'firmdeath_firms': 'sum',
   'firmdeath_estabs': 'sum',
   'firmdeath_emp': 'sum',

   # ENTRY/EXIT RATES - AVERAGE across 2 years
   'estabs_entry_rate': 'mean',
   'estabs_exit_rate': 'mean',

   # JOB FLOW COUNTS - SUM across 2 years
   'job_creation': 'sum',
   'job_creation_births': 'sum',
   'job_creation_continuers': 'sum',
   'job_destruction': 'sum',
   'job_destruction_deaths': 'sum',
   'job_destruction_continuers': 'sum',
   'net_job_creation': 'sum',

   # JOB FLOW RATES - AVERAGE across 2 years
   'job_creation_rate': 'mean',
   'job_creation_rate_births': 'mean',
   'job_destruction_rate': 'mean',
   'job_destruction_rate_deaths': 'mean',
   'net_job_creation_rate': 'mean',
   'reallocation_rate': 'mean',
}





import pandas as pd
import numpy as np
from linearmodels.panel import PanelOLS
import statsmodels.api as sm
from scipy import stats  


filepath = '../raw_data/productivity_data/total_factor_productivity.csv'
tfp_disp_df = pd.read_csv(filepath, encoding='latin1')


filepath = '../processed_data/tfp_four_digit_pivoted.csv'
manufacturing_tfp = pd.read_csv(filepath)


filepath = '../raw_data/dynamism_data/bds2022_vcn4.csv'
bds2022_vcn4 = pd.read_csv(filepath, encoding='latin1')


tfp_disp_df['naics4'] = tfp_disp_df['naics4'].astype(str)
bds2022_vcn4['vcnaics4'] = bds2022_vcn4['vcnaics4'].astype(str)

temp_df = pd.merge(
	tfp_disp_df, 
	bds2022_vcn4, 
	how='inner', 
	left_on=['naics4', 'year'], 
	right_on=['vcnaics4', 'year']
)


temp_df['naics4'] = pd.to_numeric(temp_df['naics4'], errors='raise').astype('int64')
manufacturing_tfp['NAICS'] = pd.to_numeric(manufacturing_tfp['NAICS'], errors='raise').astype('int64')

temp_df['year'] = temp_df['year'].astype(int)
manufacturing_tfp['year'] = pd.to_numeric(manufacturing_tfp['year'], errors='coerce').astype('Int64')
manufacturing_tfp = manufacturing_tfp.dropna(subset=['year'])
manufacturing_tfp['year'] = manufacturing_tfp['year'].astype(int)

merged_df = pd.merge(
    temp_df,
    manufacturing_tfp,
    how='inner',
    left_on = ['naics4', 'year'],
    right_on= ['NAICS',  'year']
)


merged_df['tfp_pct_change'] = pd.to_numeric(
    merged_df['tfp_pct_change'],
    errors='coerce'
)
merged_df['tfp_index_2017'] = pd.to_numeric(
    merged_df['tfp_index_2017'],
    errors='coerce')


columns_to_convert = [
    # Dispersion measures
    'd9990', 'd1001', 'd9990*', 'd1001*',
    
    # Entry/exit counts
    'estabs_entry', 'estabs_exit',
    'firmdeath_firms', 'firmdeath_estabs', 'firmdeath_emp',
    
    # Entry/exit rates  
    'estabs_entry_rate', 'estabs_exit_rate',
    
    # Job flow counts
    'job_creation_births', 'job_creation_continuers',
    'job_destruction_deaths', 'job_destruction_continuers',
    
    # Job flow rates
    'job_creation_rate_births', 'job_destruction_rate_deaths'
]

print("Converting object columns to numeric...")
for col in columns_to_convert:
    if col in merged_df.columns and merged_df[col].dtype == 'object':
        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')
        nan_count = merged_df[col].isna().sum()
        if nan_count > 0:
            print(f"  {col}: converted ({nan_count} NaN values created)")
        else:
            print(f"  {col}: converted successfully")


agg_dict = {
   # DISPERSION MEASURES - AVERAGE within 2-year window
   'sd': 'mean',
   'd7525': 'mean',
   'd9010': 'mean',
   'd9990': 'mean',
   'd1001': 'mean',
   'sd*': 'mean',
   'd7525*': 'mean',
   'd9010*': 'mean',
   'd9990*': 'mean',
   'd1001*': 'mean',

   # TFP metrics
   'tfp_pct_change': 'mean',     # average % change over the 2-year window
   'tfp_index_2017': 'last',     # index level at the end of the window

   # STOCK VARIABLES - LAST value (end of period snapshot)
   'firms': 'last',
   'estabs': 'last',
   'emp': 'last',
   'denom': 'last',

   # ENTRY/EXIT COUNTS - SUM across 2 years
   'estabs_entry': 'sum',
   'estabs_exit': 'sum',
   'firmdeath_firms': 'sum',
   'firmdeath_estabs': 'sum',
   'firmdeath_emp': 'sum',

   # ENTRY/EXIT RATES - AVERAGE across 2 years
   'estabs_entry_rate': 'mean',
   'estabs_exit_rate': 'mean',

   # JOB FLOW COUNTS - SUM across 2 years
   'job_creation': 'sum',
   'job_creation_births': 'sum',
   'job_creation_continuers': 'sum',
   'job_destruction': 'sum',
   'job_destruction_deaths': 'sum',
   'job_destruction_continuers': 'sum',
   'net_job_creation': 'sum',

   # JOB FLOW RATES - AVERAGE across 2 years
   'job_creation_rate': 'mean',
   'job_creation_rate_births': 'mean',
   'job_destruction_rate': 'mean',
   'job_destruction_rate_deaths': 'mean',
   'net_job_creation_rate': 'mean',
   'reallocation_rate': 'mean',
}








import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import numpy as np


data_path = "../raw_data/dynamism_data/bds2022_sec.csv"
merged = pd.read_csv(data_path)


merged = merged.rename(columns={'sector': 'NAICS'})


merged['firms_percent_destroyed'] = (merged['firmdeath_firms'] / merged['firms']) * 100


merged.columns


vars = [
    'job_creation_rate',
    'job_destruction_rate',
    'reallocation_rate', 
    'estabs_entry_rate', 
    'estabs_exit_rate', 
    'firms_percent_destroyed'
]

for var in vars:
    merged[f'{var}_3yr_avg'] = (
        merged
          .sort_values(['NAICS','year'])
          .groupby('NAICS')[var]
          .rolling(window=3, min_periods=1, center=True)
          .mean()
          .reset_index(level=0, drop=True)
    )


df_plot = (
    merged
      .groupby('year')[ 'job_creation_rate_3yr_avg' ]
      .mean()
      .reset_index()
)


X = df_plot[['year']].values
y = df_plot['job_creation_rate_3yr_avg'].values
model = LinearRegression().fit(X, y)
y_trend = model.predict(X)

plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

ax.scatter(
    df_plot['year'],
    y,
    s=50,
    color='grey',
    alpha=0.5,
)

ax.plot(
    df_plot['year'],
    y_trend,
    linestyle='--',
    linewidth=2,
    color='#6CA0DC',
)

min_year = df_plot['year'].min()
max_year = df_plot['year'].max()
start_year = min_year - (min_year % 5)
if start_year < min_year:
    start_year += 5
    
x_ticks = range(start_year, max_year + 6, 5)

ax.set_xticks(x_ticks)
ax.set_ylim(10, 22)
y_ticks = range(10, 24, 2) 
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}') 
ax.set_xlim(min_year - 1, max_year + 1)

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8) 
ax.spines['bottom'].set_linewidth(0.8)  

ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True) 

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Job creation rate (%), 3-year MA', fontsize=12)

ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout()
plt.savefig('../figures/job_creation_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


df_plot = (
    merged
      .groupby('year')[ 'job_destruction_rate_3yr_avg' ]
      .mean()
      .reset_index()
)


X = df_plot[['year']].values
y = df_plot['job_destruction_rate_3yr_avg'].values
model = LinearRegression().fit(X, y)
y_trend = model.predict(X)

plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

ax.scatter(
    df_plot['year'],
    y,
    s=50,
    color='grey',
    alpha=0.5,
)

ax.plot(
    df_plot['year'],
    y_trend,
    linestyle='--',
    linewidth=2,
    color='#E41A1C',  
)

min_year = df_plot['year'].min()
max_year = df_plot['year'].max()
start_year = min_year - (min_year % 5)
if start_year < min_year:
    start_year += 5
x_ticks = range(start_year, max_year + 6, 5)
ax.set_xticks(x_ticks)

ax.set_ylim(10, 22)

y_ticks = range(10, 24, 2) 
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}')  
ax.set_xlim(min_year - 1, max_year + 1)

# 9) Single-axis "spine" style
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8)  
ax.spines['bottom'].set_linewidth(0.8)  

ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True)  

# 11) Labels only on the axes (no title, no legend)
ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Job destruction rate (%), 3-year MA', fontsize=12)

ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout()
plt.savefig('../figures/job_destruction_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


df_plot = (
    merged
      .groupby('year')[ 'reallocation_rate_3yr_avg' ]
      .mean()
      .reset_index()
)


X = df_plot[['year']].values
y = df_plot['reallocation_rate_3yr_avg'].values
model = LinearRegression().fit(X, y)
y_trend = model.predict(X)

plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

ax.scatter(
    df_plot['year'],
    y,
    s=50,
    color='grey',
    alpha=0.5,
)

ax.plot(
    df_plot['year'],
    y_trend,
    linestyle='--',
    linewidth=2,
    color='#E41A1C')

y_min = np.floor(y.min())
y_max = np.ceil(y.max())
y_range = y_max - y_min
 
if y_range <= 5:
    tick_spacing = 0.5  # For small ranges
elif y_range <= 10:
    tick_spacing = 1.0  # For medium ranges
else:
    tick_spacing = 2.0  # For large ranges

y_ticks = np.arange(y_min, y_max + tick_spacing, tick_spacing)
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}')  # show one decimal place

ax.set_xticks(range(1985, df_plot['year'].max()+1, 5))

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

ax.grid(which='major', linestyle=':', linewidth=0.7, alpha=0.7)
ax.set_axisbelow(True)

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Job reallocation rate (%), 3-year MA', fontsize=12)

plt.tight_layout()
plt.savefig('../figures/job_reallocation_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


print(merged.columns)


df_plot = (
    merged
    .groupby('year')
    .agg({
        'estabs_exit_rate_3yr_avg': 'mean'  # or the appropriate column name for firm death rate
    })
    .reset_index()
)


plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

X = df_plot[['year']].values
years = df_plot['year']

ax.scatter(
    years,
    df_plot['estabs_exit_rate_3yr_avg'],
    s=50,
    color='grey',
    alpha=0.5
)

y_estab = df_plot['estabs_exit_rate_3yr_avg'].values
model_estab = LinearRegression().fit(X, y_estab)
y_estab_trend = model_estab.predict(X)

ax.plot(
    years,
    y_estab_trend,
    linestyle='--',
    linewidth=2,
    color='#E41A1C'  
)

min_year = min(years)
max_year = max(years)

start_year = 1980
x_ticks = range(start_year, max_year + 6, 5)
ax.set_xticks(x_ticks)
ax.set_xlim(min_year - 1, max_year + 1) 
ax.set_ylim(8, 15)

y_ticks = np.arange(8, 15.1, 1.0) 
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}')  

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Establishment Exit Rate (%), 3-year MA', fontsize=12)

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8) 
ax.spines['bottom'].set_linewidth(0.8) 
ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True) 
ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout()
plt.savefig('../figures/estabs_exit_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


df_plot = (
    merged
    .groupby('year')
    .agg({
        'estabs_entry_rate_3yr_avg': 'mean'  # or the appropriate column name for firm death rate
    })
    .reset_index()
)


plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

X = df_plot[['year']].values
years = df_plot['year']

ax.scatter(
    years,
    df_plot['estabs_entry_rate_3yr_avg'],
    s=50,
    color='grey',
    alpha=0.5
)

y_estab = df_plot['estabs_entry_rate_3yr_avg'].values
model_estab = LinearRegression().fit(X, y_estab)
y_estab_trend = model_estab.predict(X)

ax.plot(
    years,
    y_estab_trend,
    linestyle='--',
    linewidth=2,
    color='#6CA0DC'
)

min_year = min(years)
max_year = max(years)

start_year = 1980
x_ticks = range(start_year, max_year + 6, 5)
ax.set_xticks(x_ticks)
ax.set_xlim(min_year - 1, max_year + 1)  

ax.set_ylim(8, 15)

y_ticks = np.arange(8, 15.1, 1.0)  # 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}') 

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Establishment Entry Rate (%), 3-year MA', fontsize=12)

# Improve spine appearance
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8)
ax.spines['bottom'].set_linewidth(0.8)

# Improve gridlines (lighter, horizontal only)
ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True)

ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout()
plt.savefig('../figures/estabs_entry_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


df_plot = (
    merged
    .groupby('year')
    .agg({
        'firms_percent_destroyed_3yr_avg': 'mean'  # or the appropriate column name for firm death rate
    })
    .reset_index()
)


plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

X = df_plot[['year']].values
years = df_plot['year']

ax.scatter(
    years,
    df_plot['firms_percent_destroyed_3yr_avg'],
    s=50,
    color='grey',
    alpha=0.5
)

y_firm = df_plot['firms_percent_destroyed_3yr_avg'].values
model_firm = LinearRegression().fit(X, y_firm)
y_firm_trend = model_firm.predict(X)

ax.plot(
    years,
    y_firm_trend,
    linestyle='--',
    linewidth=2,
    color='#E41A1C' 
)

min_year = min(years)
max_year = max(years)

start_year = 1980
x_ticks = range(start_year, max_year + 6, 5)
ax.set_xticks(x_ticks)
ax.set_xlim(min_year - 1, max_year + 1)  

ax.set_ylim(6, 14)

y_ticks = np.arange(6, 14.1, 1.0)  
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}')  

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Firm Death Rate (%), 3-year MA', fontsize=12)

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8)  
ax.spines['bottom'].set_linewidth(0.8) 

ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True)  # Ensure grid is behind data


ax.tick_params(axis='both', which='major', labelsize=10)


plt.tight_layout()
plt.savefig('../figures/firm_death_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


data_path = "../raw_data/dynamism_data/new_firms_df.csv"
new_firms = pd.read_csv(data_path)



new_firms['new_firm_rate_3yr_avg'] = (
    new_firms
    .sort_values('year')
    .new_firm_rate
    .rolling(window=3, min_periods=1, center=True)
    .mean()
)

X = new_firms[['year']].values
y = new_firms['new_firm_rate_3yr_avg'].values

model = LinearRegression().fit(X, y)
y_trend = model.predict(X)

plt.style.use('default')
fig, ax = plt.subplots(figsize=(10, 5), dpi=120)
ax.set_facecolor('white')

ax.scatter(
    new_firms['year'],
    y,
    s=50,
    color='grey',
    alpha=0.5,
)

ax.plot(
    new_firms['year'],
    y_trend,
    linestyle='--',
    linewidth=2,
    color='#6CA0DC',
)

min_year = new_firms['year'].min()
max_year = new_firms['year'].max()

start_year = 1980
x_ticks = range(start_year, int(max_year) + 6, 5)
ax.set_xticks(x_ticks)
ax.set_xlim(min_year - 1, max_year + 1)  # Add padding

ax.set_ylim(6, 14)

y_ticks = np.arange(6, 14.1, 1.0) 
ax.set_yticks(y_ticks)
ax.yaxis.set_major_formatter('{x:.1f}')  

ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_linewidth(0.8)  
ax.spines['bottom'].set_linewidth(0.8)  

ax.grid(which='major', axis='y', linestyle='-', linewidth=0.4, color='#E0E0E0', alpha=0.7)
ax.set_axisbelow(True)  

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Firm birth rate (%), 3-year MA', fontsize=12)

ax.tick_params(axis='both', which='major', labelsize=10)

plt.tight_layout()
plt.savefig('../figures/firm_birth_rate_trend.png', dpi=300, bbox_inches='tight')
plt.show()


vars = [
    'job_creation_rate',
    'job_destruction_rate',
    'reallocation_rate', 
    'estabs_entry_rate', 
    'estabs_exit_rate', 
    'firms_percent_destroyed'
]

for var in vars:
    merged[f'{var}_2yr_avg'] = (
        merged
          .sort_values(['NAICS','year'])
          .groupby('NAICS')[var]
          .rolling(window=2, min_periods=1, center=True)
          .mean()
          .reset_index(level=0, drop=True)
    )



def weighted_avg(values, weights):
    values = pd.to_numeric(values, errors='coerce')
    weights = pd.to_numeric(weights, errors='coerce')

    mask = ~(np.isnan(values) | np.isnan(weights))
    values = values[mask]
    weights = weights[mask]
    
    return np.average(values, weights=weights) if len(values) > 0 else np.nan

national = pd.DataFrame()
for year, group in merged.groupby("year"):
    firm_entry = weighted_avg(
        group["job_creation_rate_3yr_avg"], 
        group["weight"]
    )
    estab_entry = weighted_avg(
        group["estabs_entry_rate_3yr_avg"], 
        group["weight"]
    )
    
    national = pd.concat([
        national, 
        pd.DataFrame({
            "year": [year],
            "firm_entry_wt": [firm_entry],
            "estab_entry_wt": [estab_entry]
        })
    ])

national = national.reset_index(drop=True).sort_values("year")


fig, ax1 = plt.subplots(figsize=(10, 6))

ax1.plot(
    national["year"],
    national["firm_entry_wt"],
    linestyle="-",  
    linewidth=2.5,
    color="#4993C3",
    label="Firms"
)
ax1.set_ylabel("Firm entry rate (%)", fontsize=11)
ax1.set_xlabel("Year", fontsize=11)

y_min = 8 
y_max = 19 
ax1.set_ylim(y_min, y_max)
ax1.set_xlim(national["year"].min(), national["year"].max())

ax2 = ax1.twinx()
ax2.plot(
    national["year"],
    national["estab_entry_wt"],
    linestyle="--", 
    linewidth=2.5,
    color="#4993C3",  
    label="Establishments"
)
ax2.set_ylabel("Establishment entry rate (%)", fontsize=11)
ax2.set_ylim(y_min, y_max) 

h1, l1 = ax1.get_legend_handles_labels()
h2, l2 = ax2.get_legend_handles_labels()
ax1.legend(h1 + h2, l1 + l2, loc="upper right", frameon=False)

ax1.grid(True, linestyle='-', color='lightgray', alpha=0.5)

ax1.spines["top"].set_visible(False)
ax2.spines["top"].set_visible(False)

for spine in ["bottom", "left", "right"]:
    ax1.spines[spine].set_visible(True)
    ax2.spines[spine].set_visible(True)

ax1.set_yticks(range(8, 20, 2))  # 8, 10, 12, 14, 16, 18
ax2.set_yticks(range(8, 20, 2))  # Match left axis

plt.tight_layout()
plt.show()





import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import PercentFormatter
import os

df = pd.read_csv('../processed_data/two_digit_NAICS_bds_tfp.csv')


df.columns


df['tfp_index_2017'] = pd.to_numeric(df['tfp_index_2017'], errors='coerce')
df['tfp_pct_change'] = pd.to_numeric(df['tfp_pct_change'], errors='coerce')

df_plot = (
    df
      .groupby(['NAICS', 'year'])
      .agg({
          'tfp_index_2017': 'mean',
          'tfp_pct_change': 'mean'
      })
      .reset_index()
)

if 'weight' in df.columns:
    # Calculate average weight by NAICS and year
    weights_df = (
        df
        .groupby(['NAICS', 'year'])['weight']
        .mean()
        .reset_index()
    )
    
    df_plot = pd.merge(
        df_plot,
        weights_df,
        on=['NAICS', 'year'],
        how='left'
    )


fig, ax = plt.subplots(figsize=(12,6), dpi=120)
ax.set_facecolor('white')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(which='major', linestyle=':', linewidth=0.7, alpha=0.7)
ax.set_axisbelow(True)

cmap = plt.get_cmap('tab20')

naics_codes = df_plot['NAICS'].unique()
for i, code in enumerate(naics_codes):
    sub = df_plot[df_plot['NAICS'] == code]
    ax.plot(
        sub['year'],
        sub['tfp_index_2017'],
        label=str(code),
        color=cmap(i % 20),
        linewidth=1.2,
        alpha=0.7,
        marker='o',
        markersize=4
    )

ax.set_xticks(range(df_plot['year'].min(), df_plot['year'].max()+1, 5))
ax.set_yticks(range(80, 141, 10))
ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Index (2017 = 100)', fontsize=12)

ax.legend(
    title='NAICS',
    bbox_to_anchor=(1.02,1),
    loc='upper left',
    fontsize=8,
    frameon=False
)

plt.tight_layout()
plt.show()


df_plot.head(5)


aggregate_tfp = (
    df_plot.dropna(subset=['tfp_index_2017', 'weight'])
    .groupby('year')
    .apply(lambda x: np.average(x['tfp_index_2017'], weights=x['weight']))
    .reset_index()
    .rename(columns={0: 'weighted_tfp'})
)


aggregate_tfp.head(5)


fig, ax = plt.subplots(figsize=(12, 6), dpi=120)
ax.set_facecolor('white')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(which='major', linestyle=':', linewidth=0.7, alpha=0.7)
ax.set_axisbelow(True)

naics_codes = df_plot['NAICS'].unique()
for code in naics_codes:
    sub = df_plot[df_plot['NAICS'] == code]
    ax.plot(
        sub['year'],
        sub['tfp_index_2017'],
        color='grey',
        linewidth=0.8,
        alpha=0.3,
        marker='',  
    )

ax.plot(
    aggregate_tfp['year'],
    aggregate_tfp['weighted_tfp'],
    label='Weighted Aggregate TFP',
    color='#1f77b4', 
    linewidth=3.0,    
    alpha=1.0,
    marker='o',
    markersize=6,   
    zorder=10        
)

ax.set_xticks(range(df_plot['year'].min(), df_plot['year'].max()+1, 5))

y_min = np.floor(min(df_plot['tfp_index_2017'].min(), aggregate_tfp['weighted_tfp'].min()) / 10) * 10
y_max = np.ceil(max(df_plot['tfp_index_2017'].max(), aggregate_tfp['weighted_tfp'].max()) / 10) * 10
ax.set_yticks(range(int(y_min), int(y_max)+1, 10))

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Total Factor Productivity (2017 = 100)', fontsize=12)

os.makedirs('../figures', exist_ok=True)  
plt.tight_layout()
plt.savefig('../figures/weighted_tfp_trends.png', dpi=300, bbox_inches='tight')
plt.show()


fig, ax = plt.subplots(figsize=(12, 6), dpi=120)
ax.set_facecolor('white')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_position(('axes', 0.0))

ax.grid(which='major', axis='y', linestyle=':', linewidth=0.7, alpha=0.7)
ax.margins(x=0)
ax.set_axisbelow(True)

df_plot_clean = df_plot.dropna(subset=['tfp_index_2017'])
aggregate_tfp_clean = aggregate_tfp.dropna(subset=['weighted_tfp'])

years = aggregate_tfp_clean['year'].values
tfp_values = aggregate_tfp_clean['weighted_tfp'].values

ax.plot(
    years,
    tfp_values,
    color='#1f77b4',
    linewidth=3.0,
    alpha=1.0,
    zorder=10,
    label='Aggregate TFP'   
)

if len(years) > 0:  
    historical_start = tfp_values[0] 
    historical_values = []
    
    for year in range(int(years.min()), int(years.max())+1):
        years_from_start = year - years.min()
        historical_values.append(historical_start * (1 + 1.3/100) ** years_from_start)
    
    year_range = range(int(years.min()), int(years.max())+1)
    
    ax.plot(
        year_range,
        historical_values,
        '--',
        linewidth=2.5,
        color='#ff7f0e',
        zorder=5,
        label='Historical Trend' 
    )

    # shade in
    interp_actual = np.interp(year_range, years, tfp_values)
    ax.fill_between(
        year_range,
        interp_actual,
        historical_values,
        where=[hv > tv for hv, tv in zip(historical_values, interp_actual)],
        color='#ff7f0e',
        alpha=0.15
    )

if len(years) > 0:
    ax.set_xticks(range(int(years.min()), int(years.max())+1, 5))
    
    all_values = np.concatenate([tfp_values, historical_values])
    y_min = np.floor(min(all_values) / 10) * 10
    y_max = np.ceil(max(all_values) / 10) * 10
    ax.set_yticks(range(int(y_min), int(y_max)+1, 10))

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('Total Factor Productivity (index: 2017 = 100)', fontsize=12)
ax.legend(loc='upper left', frameon=False, fontsize=12)

os.makedirs('../figures', exist_ok=True)
plt.tight_layout()
plt.savefig('../figures/tfp_productivity_slowdown.png', dpi=300, bbox_inches='tight')
plt.show()





def plot_weighted_tfp_by_year(df):
    df = df.copy()
    
    # Convert columns to numeric, replacing non-numeric values with NaN
    df['tfp_pct_change'] = pd.to_numeric(df['tfp_pct_change'], errors='coerce')
    df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
    
    # Drop rows with NaN in these columns
    df = df.dropna(subset=['tfp_pct_change', 'weight'])
    
    # Calculate weighted average manually by year
    yearly_data = []
    for year in sorted(df['year'].unique()):
        year_data = df[df['year'] == year]
        
        # Manual weighted average calculation
        weighted_sum = (year_data['tfp_pct_change'] * year_data['weight']).sum()
        weight_sum = year_data['weight'].sum()
        
        if weight_sum > 0:  # Avoid division by zero
            weighted_avg = weighted_sum / weight_sum
            yearly_data.append({'year': year, 'weighted_tfp_change': weighted_avg})
    
    yearly_tfp = pd.DataFrame(yearly_data)
    
    # Print the data we'll be plotting
    print("Data to be plotted:")
    print(yearly_tfp)
    
    # Create the plot with adequate figure size
    plt.figure(figsize=(14, 7))
    
    # Get x positions - use integers for bar positions
    x_pos = list(range(len(yearly_tfp)))
    
    # Create the bars using integer positions (no multiplication by 100)
    bars = plt.bar(x_pos, yearly_tfp['weighted_tfp_change'])
    
    # Color the bars based on positive/negative values
    for i, bar in enumerate(bars):
        if yearly_tfp['weighted_tfp_change'].iloc[i] < 0:
            bar.set_color('tomato')
        else:
            bar.set_color('mediumseagreen')
    
    # Add a horizontal line at y=0
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # Add labels and title
    plt.title('Weighted Average TFP Percent Change by Year', fontsize=14)
    plt.xlabel('Year', fontsize=12)
    plt.ylabel('TFP Percent Change (%)', fontsize=12)
    
    # Format y-axis labels with percentage sign
    from matplotlib.ticker import FuncFormatter
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.1f}%'))
    
    # Add grid lines for readability
    plt.grid(axis='y', alpha=0.3)
    
    # Set x-ticks to be at the bar positions, but labeled with years
    plt.xticks(x_pos, yearly_tfp['year'].astype(str), rotation=45)
    
    # Add data labels with adjusted positioning
    for i, v in enumerate(yearly_tfp['weighted_tfp_change']):
        # Adjust vertical offset based on value
        offset = 0.1 if v >= 0 else -0.2
        plt.text(i, v + offset, f'{v:.2f}%', ha='center', fontsize=9)
    
    # Adjust layout with specific margins
    plt.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9)
    
    # Show the plot
    plt.show()
    
    return yearly_tfp

# Example usage:
yearly_tfp = plot_weighted_tfp_by_year(df)


def plot_weighted_tfp_line(df):
    """
    Create a line plot of weighted average TFP percent change by year.
    Values in tfp_pct_change are already in percentage form.
    """
    # Make a copy to avoid modifying the original
    df = df.copy()
    
    # Convert columns to numeric, replacing non-numeric values with NaN
    df['tfp_pct_change'] = pd.to_numeric(df['tfp_pct_change'], errors='coerce')
    df['weight'] = pd.to_numeric(df['weight'], errors='coerce')
    
    # Drop rows with NaN in these columns
    df = df.dropna(subset=['tfp_pct_change', 'weight'])
    
    # Calculate weighted average manually by year
    yearly_data = []
    for year in sorted(df['year'].unique()):
        year_data = df[df['year'] == year]
        
        # Manual weighted average calculation
        weighted_sum = (year_data['tfp_pct_change'] * year_data['weight']).sum()
        weight_sum = year_data['weight'].sum()
        
        if weight_sum > 0:  # Avoid division by zero
            weighted_avg = weighted_sum / weight_sum
            yearly_data.append({'year': year, 'weighted_tfp_change': weighted_avg})
    
    yearly_tfp = pd.DataFrame(yearly_data)
    
    # Print the data we'll be plotting
    print("Data to be plotted:")
    print(yearly_tfp)
    
    # Create the plot with adequate figure size
    plt.figure(figsize=(14, 7))
    
    # Create a light blue line plot with markers
    plt.plot(
        yearly_tfp['year'], 
        yearly_tfp['weighted_tfp_change'],
        marker='o',                # Add circle markers at each data point
        linestyle='-',            # Solid line
        linewidth=2.5,            # Slightly thicker line
        color='skyblue',          # Light blue color
        markerfacecolor='white',  # White fill for markers
        markeredgecolor='skyblue', # Light blue edge for markers
        markeredgewidth=2,        # Slightly thicker marker edge
        markersize=8              # Marker size
    )
    
    # Add a horizontal line at y=0
    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
    
    # Fill area between line and y=0 with light colors
    plt.fill_between(
        yearly_tfp['year'],
        yearly_tfp['weighted_tfp_change'],
        0,
        where=(yearly_tfp['weighted_tfp_change'] >= 0),
        color='skyblue',
        alpha=0.3
    )
    plt.fill_between(
        yearly_tfp['year'],
        yearly_tfp['weighted_tfp_change'],
        0,
        where=(yearly_tfp['weighted_tfp_change'] < 0),
        color='lightcoral',
        alpha=0.3
    )
    
    # Add labels and title
    plt.title('Weighted Average TFP Percent Change by Year', fontsize=16)
    plt.xlabel('Year', fontsize=12)
    plt.ylabel('TFP Percent Change (%)', fontsize=12)
    
    # Format y-axis labels with percentage sign
    plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'{x:.1f}%'))
    
    # Add grid lines for readability
    plt.grid(True, alpha=0.3)
    
    # Set x-axis tick frequency - show fewer years if there are many
    num_years = len(yearly_tfp)
    step = max(1, num_years // 10)  # Show about 10 year labels
    
    plt.xticks(
        yearly_tfp['year'][::step],  # Show every nth year
        yearly_tfp['year'][::step],
        rotation=45
    )
    
    # Add data labels for each point
    for i, row in yearly_tfp.iterrows():
        x = row['year']
        y = row['weighted_tfp_change']
        
        # Position labels above positive values, below negative values
        offset = 0.15 if y >= 0 else -0.25
        
        plt.annotate(
            f'{y:.2f}%',
            (x, y),
            textcoords="offset points", 
            xytext=(0, 10 if y >= 0 else -20),
            ha='center',
            fontsize=9,
            bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7)
        )
    
    # Adjust plot limits to make room for labels
    plt.margins(x=0.02, y=0.1)
    
    # Adjust layout with specific margins
    plt.tight_layout()
    
    # Show the plot
    plt.show()
    
    return yearly_tfp

# Example usage:
yearly_tfp = plot_weighted_tfp_line(df)



# 4) Set up the figure
fig, ax = plt.subplots(figsize=(12, 6), dpi=120)
ax.set_facecolor('white')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.grid(which='major', linestyle=':', linewidth=0.7, alpha=0.7)
ax.set_axisbelow(True)

# 5) Plot all NAICS trends in grey (semi-transparent)
naics_codes = df_plot['NAICS'].unique()
for code in naics_codes:
    sub = df_plot[df_plot['NAICS'] == code]
    ax.plot(
        sub['year'],
        sub['tfp_pct_change'],  # Changed to tfp_pct_change
        color='grey',
        linewidth=0.8,
        alpha=0.3,
        marker='',  # No markers for individual series to reduce clutter
    )

# 6) Plot the weighted aggregate TFP growth in blue (bold)
# First, calculate the weighted aggregate TFP growth
aggregate_growth = df_plot.copy()
if 'weight' in aggregate_growth.columns:
    # Ensure weight is normalized within each year
    aggregate_growth['norm_weight'] = aggregate_growth.groupby('year')['weight'].transform(
        lambda x: x / x.sum()
    )
    
    # Calculate weighted average by year
    weighted_growth = (
        aggregate_growth
        .groupby('year')
        .apply(lambda x: (x['tfp_pct_change'] * x['norm_weight']).sum())  # Changed to tfp_pct_change
        .reset_index(name='weighted_tfp_growth')  # Renamed to weighted_tfp_growth
    )
else:
    # If no weights, just take simple average
    weighted_growth = (
        df_plot
        .groupby('year')['tfp_pct_change']  # Changed to tfp_pct_change
        .mean()
        .reset_index(name='weighted_tfp_growth')  # Renamed to weighted_tfp_growth
    )

# Plot the weighted aggregate growth
ax.plot(
    weighted_growth['year'],
    weighted_growth['weighted_tfp_growth'],
    label='Weighted Aggregate TFP Growth',  # Updated label
    color='#1f77b4',  # Blue
    linewidth=3.0,
    alpha=1.0,
    marker='o',
    markersize=6,
    zorder=10
)

# 7) Ticks & labels
ax.set_xticks(range(df_plot['year'].min(), df_plot['year'].max()+1, 5))

# Dynamic y-ticks based on data range (for percentage changes)
y_min = np.floor(min(df_plot['tfp_pct_change'].min(), weighted_growth['weighted_tfp_growth'].min()))
y_max = np.ceil(max(df_plot['tfp_pct_change'].max(), weighted_growth['weighted_tfp_growth'].max()))
# Add some padding to y-axis
y_padding = (y_max - y_min) * 0.1
y_min -= y_padding
y_max += y_padding
ax.set_ylim(y_min, y_max)

# For percentage changes, we might want different tick spacing
tick_spacing = max(1, int((y_max - y_min) / 10))  # Aim for about 10 ticks
ax.set_yticks(np.arange(int(y_min), int(y_max)+1, tick_spacing))

ax.set_xlabel('Year', fontsize=12)
ax.set_ylabel('TFP Growth Rate (%)', fontsize=12)  # Updated y-axis label
ax.set_title('Weighted Aggregate TFP Growth Trend', fontsize=14)  # Updated title

# Add horizontal line at y=0 to highlight positive vs negative growth
ax.axhline(y=0, color='black', linestyle='-', alpha=0.3, linewidth=1)

# Add legend
ax.legend(loc='best', frameon=True, facecolor='white', framealpha=0.8)

# Save with relative path
plt.tight_layout()
plt.savefig('../figures/weighted_tfp_growth_trends.png', dpi=300, bbox_inches='tight')
plt.show()








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.tsa.stattools import grangercausalitytests
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from itertools import product


df = pd.read_csv('../processed_data/three_digit_NAICS_final.csv')


num_industries = df['Industry'].nunique()
print(num_industries)


df = df.sort_values(['Industry', 'year'])

df['tfp_ann_pct_lead'] = df.groupby('Industry')['tfp_ann_pct'].shift(-1)
df['tfp_pct_change_lead'] = df.groupby('Industry')['tfp_pct_change'].shift(-1)






df = df.sort_values(['Industry', 'year'])

mod1 = smf.ols(
    'tfp_ann_pct_lead ~ eer',
    data=df
).fit(cov_type='HC1')          

print(mod1.summary())



df = df.sort_values(['Industry', 'year'])

dependent_vars = [
    'tfp_ann_pct_lead', 
    'tfp_pct_change_lead', 
    'tfp_diff3', 
    'tfp_ann_pct', 
    'tfp_log', 
    'tfp_index_2017'
]

independent_vars = [
    'eer', 
    'pct_high_growth_emp', 
    'reallocation_rate'
]

results = {}

print("="*80)
print("SIMPLE OLS REGRESSION RESULTS")
print("="*80)

for dep_var in dependent_vars:
    for indep_var in independent_vars:
        
        # Create the formula
        formula = f'{dep_var} ~ {indep_var}'
        
        try:
            # Run the regression
            model = smf.ols(formula, data=df).fit(cov_type='HC1')
            
            # Store results
            key = f'{dep_var}_vs_{indep_var}'
            results[key] = model
            
            # Print summary information
            print(f"\n{'-'*60}")
            print(f"MODEL: {dep_var} ~ {indep_var}")
            print(f"{'-'*60}")
            print(f"R-squared: {model.rsquared:.4f}")
            print(f"Adj. R-squared: {model.rsquared_adj:.4f}")
            print(f"F-statistic: {model.fvalue:.4f}")
            print(f"Prob (F-statistic): {model.f_pvalue:.4f}")
            print(f"N observations: {model.nobs:.0f}")
            
            # Print coefficient information
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]  # Robust standard errors
            t_stat = model.tvalues[indep_var]
            p_value = model.pvalues[indep_var]
            
            print(f"\nCoefficient on {indep_var}:")
            print(f"  Estimate: {coef:.6f}")
            print(f"  Std Error: {se:.6f}")
            print(f"  t-statistic: {t_stat:.4f}")
            print(f"  p-value: {p_value:.4f}")
            print(f"  Significance: {'***' if p_value < 0.01 else '**' if p_value < 0.05 else '*' if p_value < 0.10 else ''}")
            
        except Exception as e:
            print(f"\nERROR with {dep_var} ~ {indep_var}: {str(e)}")
            continue

print("\n" + "="*80)
print("SUMMARY TABLE OF ALL REGRESSIONS")
print("="*80)

summary_data = []
for dep_var in dependent_vars:
    for indep_var in independent_vars:
        key = f'{dep_var}_vs_{indep_var}'
        if key in results:
            model = results[key]
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]
            p_val = model.pvalues[indep_var]
            r2 = model.rsquared
            n_obs = model.nobs
            
            summary_data.append({
                'Dependent Variable': dep_var,
                'Independent Variable': indep_var,
                'Coefficient': f"{coef:.6f}",
                'Std Error': f"{se:.6f}",
                'p-value': f"{p_val:.4f}",
                'R-squared': f"{r2:.4f}",
                'N': f"{n_obs:.0f}",
                'Significance': '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*' if p_val < 0.10 else ''
            })

summary_df = pd.DataFrame(summary_data)
print(summary_df.to_string(index=False))

summary_df.to_csv('/Users/danielseymour/Developer/EC334-Summative/processed_data/ols_regression_results.csv', index=False)
print(f"\nResults saved to: /Users/danielseymour/Developer/EC334-Summative/processed_data/ols_regression_results.csv")


data = df[['tfp_ann_pct','pct_high_growth_emp']].dropna()
grangercausalitytests(data[['pct_high_growth_emp','tfp_ann_pct']], maxlag=3)





# ------------------------------------------------------------------
# 3.2  Year OR industry fixed effects, cluster by industry
# ------------------------------------------------------------------
# tfp_ann_pct_lead, tfp_pct_change_lead, tfp_diff3, tfp_ann_pct, tfp_log, tfp_index_2017
# eer, pct_high_growth_emp, reallocation_rate

# 1. First, check for missing values in the data
missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

# 2. Drop missing values explicitly to ensure consistency
df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

# 3. Run the regression with the clean data
mod2 = smf.ols(
    'tfp_ann_pct_lead ~ pct_high_growth_emp + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


missing_values = df[['tfp_pct_change_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

mod2 = smf.ols(
    'tfp_pct_change_lead ~ pct_high_growth_emp + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

mod2 = smf.ols(
    'tfp_ann_pct_lead ~ reallocation_rate + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


missing_values = df[['tfp_ann_pct_lead']].isnull().sum()
print("Missing values in each column:")
print(missing_values)

df_clean = df.dropna(subset=['tfp_ann_pct_lead'])
print(f"Original dataframe shape: {df.shape}, Clean dataframe shape: {df_clean.shape}")

mod2 = smf.ols(
    'tfp_ann_pct_lead ~ eer + C(year)',
    data=df_clean
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_clean['year']}
)

print(mod2.summary())


df = df.sort_values(['Industry', 'year'])

# Define dependent and independent variables
dependent_vars = [
    'tfp_ann_pct_lead', 
    'tfp_pct_change_lead', 
    'tfp_diff3', 
    'tfp_ann_pct', 
    'tfp_log', 
    'tfp_index_2017'
]

independent_vars = [
    'eer', 
    'pct_high_growth_emp', 
    'reallocation_rate'
]

results_fe = {}

print("="*80)
print("OLS REGRESSION RESULTS WITH INDUSTRY FIXED EFFECTS")
print("="*80)

for dep_var in dependent_vars:
    for indep_var in independent_vars:
        
        formula = f'{dep_var} ~ {indep_var} + C(Industry)'
        
        try:
            model = smf.ols(formula, data=df).fit(cov_type='HC1')
            
            key = f'{dep_var}_vs_{indep_var}_FE'
            results_fe[key] = model
            
            # Print summary information
            print(f"\n{'-'*60}")
            print(f"MODEL: {dep_var} ~ {indep_var} + Industry FE")
            print(f"{'-'*60}")
            print(f"R-squared: {model.rsquared:.4f}")
            print(f"Adj. R-squared: {model.rsquared_adj:.4f}")
            print(f"F-statistic: {model.fvalue:.4f}")
            print(f"Prob (F-statistic): {model.f_pvalue:.4f}")
            print(f"N observations: {model.nobs:.0f}")
            
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]  # Robust standard errors
            t_stat = model.tvalues[indep_var]
            p_value = model.pvalues[indep_var]
            
            print(f"\nCoefficient on {indep_var}:")
            print(f"  Estimate: {coef:.6f}")
            print(f"  Std Error: {se:.6f}")
            print(f"  t-statistic: {t_stat:.4f}")
            print(f"  p-value: {p_value:.4f}")
            print(f"  Significance: {'***' if p_value < 0.01 else '**' if p_value < 0.05 else '*' if p_value < 0.10 else ''}")
            
            industry_params = [param for param in model.params.index if 'C(Industry)' in param]
            print(f"  Number of Industry FE: {len(industry_params)}")
            
        except Exception as e:
            print(f"\nERROR with {dep_var} ~ {indep_var} + Industry FE: {str(e)}")
            continue

print("\n" + "="*80)
print("SUMMARY TABLE OF ALL REGRESSIONS WITH INDUSTRY FIXED EFFECTS")
print("="*80)

summary_data_fe = []
for dep_var in dependent_vars:
    for indep_var in independent_vars:
        key = f'{dep_var}_vs_{indep_var}_FE'
        if key in results_fe:
            model = results_fe[key]
            coef = model.params[indep_var]
            se = model.HC1_se[indep_var]
            p_val = model.pvalues[indep_var]
            r2 = model.rsquared
            adj_r2 = model.rsquared_adj
            n_obs = model.nobs
            
            # Count industry fixed effects
            industry_params = [param for param in model.params.index if 'C(Industry)' in param]
            n_industries = len(industry_params)
            
            summary_data_fe.append({
                'Dependent Variable': dep_var,
                'Independent Variable': indep_var,
                'Coefficient': f"{coef:.6f}",
                'Std Error': f"{se:.6f}",
                'p-value': f"{p_val:.4f}",
                'R-squared': f"{r2:.4f}",
                'Adj R-squared': f"{adj_r2:.4f}",
                'N': f"{n_obs:.0f}",
                'Industry FE': f"{n_industries}",
                'Significance': '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*' if p_val < 0.10 else ''
            })

summary_df_fe = pd.DataFrame(summary_data_fe)
print(summary_df_fe.to_string(index=False))

summary_df_fe.to_csv('/Users/danielseymour/Developer/EC334-Summative/processed_data/ols_industry_fe_results.csv', index=False)
print(f"\nResults saved to: /Users/danielseymour/Developer/EC334-Summative/processed_data/ols_industry_fe_results.csv")

print(f"\nTo access individual models, use: results_fe['dependent_var_vs_independent_var_FE']")
print(f"Example: results_fe['tfp_ann_pct_lead_vs_eer_FE'].summary()")

print(f"\n" + "="*80)
print("EXAMPLE: Full summary for tfp_ann_pct_lead ~ eer + Industry FE")
print("="*80)
if 'tfp_ann_pct_lead_vs_eer_FE' in results_fe:
    print(results_fe['tfp_ann_pct_lead_vs_eer_FE'].summary())

print(f"\n" + "="*80)
print("INDUSTRY FIXED EFFECTS INCLUDED")
print("="*80)
if 'tfp_ann_pct_lead_vs_eer_FE' in results_fe:
    model_example = results_fe['tfp_ann_pct_lead_vs_eer_FE']
    industry_effects = [param for param in model_example.params.index if 'C(Industry)' in param]
    print(f"Total number of industry fixed effects: {len(industry_effects)}")
    
    # Show a few examples of the industry coefficients
    print(f"\nExample industry fixed effect coefficients:")
    for i, effect in enumerate(industry_effects[:5]):  # Show first 5
        coef = model_example.params[effect]
        print(f"  {effect}: {coef:.6f}")
    if len(industry_effects) > 5:
        print(f"  ... and {len(industry_effects) - 5} more industry fixed effects")

print(f"\n" + "="*80)
print("NOTE: COMPARISON WITH SIMPLE OLS")
print("="*80)
print("The industry fixed effects control for time-invariant industry characteristics.")
print("Compare R-squared values with the simple OLS results to see the improvement.")
print("The coefficient on your main variables now represents within-industry variation.")





resid = mod2.resid
out = df_clean.copy()
out['resid'] = resid

out = out.sort_values(['Industry','year'])
out['resid_lag1'] = out.groupby('Industry')['resid'].shift(1)
out['d_resid']    = out['resid'] - out['resid_lag1']

test_df = out.dropna(subset=['resid_lag1','d_resid'])

aux = sm.OLS(test_df['d_resid'], sm.add_constant(test_df['resid_lag1'])).fit()

print(aux.summary())


df_clean['tfp_ann_pct_lead_lag1'] = (
    df_clean
    .sort_values(['Industry','year'])
    .groupby('Industry')['tfp_ann_pct_lead']
    .shift(1)
)

df_dyn = df_clean.dropna(subset=['tfp_ann_pct_lead_lag1', 'tfp_ann_pct_lead'])

mod_ar1 = smf.ols(
    'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + C(year)',
    data=df_dyn
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_dyn['NAICS']}
)

print("AR(1) Model - TFP with its own lag:")
print(mod_ar1.summary())



df_clean['tfp_ann_pct_lead_lag1'] = (
    df_clean
    .sort_values(['Industry','year'])
    .groupby('Industry')['tfp_ann_pct_lead']
    .shift(1)
)
df_dyn = df_clean.dropna(subset=['tfp_ann_pct_lead_lag1', 'tfp_ann_pct_lead'])

mod_dyn = smf.ols(
    'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + eer + C(year)',
    data=df_dyn
).fit(
    cov_type='cluster',
    cov_kwds={'groups': df_dyn['NAICS']}    # cluster by industry
)

print(mod_dyn.summary())


dyn_vars = ['eer', 'pct_high_growth_emp', 'reallocation_rate']

for var in dyn_vars:
    df_clean[f'{var}_lag1'] = (
        df_clean
        .sort_values(['Industry','year'])
        .groupby('Industry')[var]
        .shift(1)
    )

lag_vars = ['tfp_ann_pct_lead_lag1'] + [f'{var}_lag1' for var in dyn_vars]
df_dyn = df_clean.dropna(subset=lag_vars + ['tfp_ann_pct_lead'])



for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ TFP_lag + {var}_lag")
    print(f"{'='*60}")
    
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', 'tfp_ann_pct_lead_lag1', f'{var}_lag1'])
    
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ tfp_ann_pct_lead_lag1 + {var}_lag1 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    print(mod_bivariate.summary())


for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ TFP_lag + {var}_lag")
    print(f"{'='*60}")
    
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', 'tfp_ann_pct_lead_lag1', f'{var}_lag1'])
    
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ {var}_lag1 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    print(mod_bivariate.summary())


for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate: TFP ~ {var}_lag1 + {var}_lag2")
    print(f"{'='*60}")
    
    df_bivariate = df_clean.dropna(subset=['tfp_ann_pct_lead', f'{var}_lag1', f'{var}_lag2'])
    
    mod_bivariate = smf.ols(
        f'tfp_ann_pct_lead ~ {var}_lag1 + {var}_lag2 + C(year)',
        data=df_bivariate
    ).fit(
        cov_type='cluster',
        cov_kwds={'groups': df_bivariate['NAICS']}
    )
    
    print(f"R-squared: {mod_bivariate.rsquared:.4f}")
    print(f"N: {mod_bivariate.nobs}")
    print(f"\nCoefficients of interest:")
    print(f"{var}_lag1: {mod_bivariate.params[f'{var}_lag1']:.4f} (p={mod_bivariate.pvalues[f'{var}_lag1']:.4f})")
    print(f"{var}_lag2: {mod_bivariate.params[f'{var}_lag2']:.4f} (p={mod_bivariate.pvalues[f'{var}_lag2']:.4f})")





early_years = (1988, 1992)  
late_years  = (2015, 2019)

dyn_vars = ['eer','estabs_entry_rate', 'reallocation_rate', 'pct_high_growth_emp']
prod_var = 'tfp_index_2017'

early = (df['year'].between(*early_years))
late  = (df['year'].between(*late_years))

common = dyn_vars + [prod_var, 'weight']

early_means = (
    df[early]
      .groupby('NAICS')[ common ]
      .mean()
      .rename(columns=lambda c: f"{c}_early")
)

late_means = (
    df[late]
      .groupby('NAICS')[ common ]
      .mean()
      .rename(columns=lambda c: f"{c}_late")
)

wide = early_means.join(late_means, how='inner')


for v in dyn_vars:
    wide[f"long_run_diff_log_{v}"] = np.log(wide[f"{v}_late"]) - np.log(wide[f"{v}_early"])


wide["long_run_diff_log_TFP"] = np.log(wide["tfp_index_2017_late"]) - np.log(wide["tfp_index_2017_early"])


wide.head()


df.columns


wide.columns


from statsmodels.stats.outliers_influence import variance_inflation_factor
vifs = pd.DataFrame({
    'var': X.columns,
    'VIF': [variance_inflation_factor(X.values, i) 
            for i in range(X.shape[1])]
})
print(vifs)


X = wide[[f"long_run_diff_log_{v}" for v in dyn_vars]]
X = sm.add_constant(X)
y = wide["long_run_diff_log_TFP"]

model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': wide.index})
print(model.summary())



for var in dyn_vars:
    print(f"\n{'='*60}")
    print(f"Bivariate regression: log TFP ~ log {var}")
    print(f"{'='*60}")
    
    X_single = wide[[f"long_run_diff_log_{var}"]]
    X_single = sm.add_constant(X_single)
    y = wide["long_run_diff_log_TFP"]
    
    model_single = sm.OLS(y, X_single).fit(cov_type='cluster', cov_kwds={'groups': wide.index})
    print(model_single.summary())


wide.columns


wide['log_weight_early']   = np.log(wide['weight_early'])
wide['log_tfp0']           = np.log(wide['tfp_index_2017_early'])

X = wide[[
    'long_run_diff_log_eer',
    'long_run_diff_log_reallocation_rate',
    'long_run_diff_log_pct_high_growth_emp',
    'log_weight_early',
]]

X = sm.add_constant(X)

y = wide['long_run_diff_log_TFP']

model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


X = wide[[
    'long_run_diff_log_eer',
    'long_run_diff_log_reallocation_rate',
    'long_run_diff_log_pct_high_growth_emp',
    'log_weight_early'
]].copy()

X['const'] = 1.0

vif_data = pd.DataFrame({
    'variable': X.columns,
    'VIF': [
        variance_inflation_factor(X.values, i)
        for i in range(X.shape[1])
    ]
})

print(vif_data)



w = wide['weight_early']
X = wide[['long_run_diff_log_eer']]      
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


wide['log_weight_early']   = np.log(wide['weight_early'])
X = wide[[
    'long_run_diff_log_eer',
    'log_weight_early',
]]

X = sm.add_constant(X)

y = wide['long_run_diff_log_TFP']

model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


wide['log_weight_early']   = np.log(wide['weight_early'])
X = wide[[
    'long_run_diff_log_pct_high_growth_emp',
]]
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


w = wide['weight_early']
X = wide[['long_run_diff_log_pct_high_growth_emp']]   
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


wide['log_weight_early']   = np.log(wide['weight_early'])

X = wide[[
    'long_run_diff_log_reallocation_rate',
]]

X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']
model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())


w = wide['weight_early']
X = wide[['long_run_diff_log_reallocation_rate']]     
X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

wls_mod = sm.WLS(y, X, weights=w).fit(cov_type='HC1')
print(wls_mod.summary())


wide['log_weight_early']   = np.log(wide['weight_early'])
wide['log_tfp0']           = np.log(wide['tfp_index_2017_early'])

X = wide[[
    'long_run_diff_log_estabs_entry_rate'
]]

X = sm.add_constant(X)
y = wide['long_run_diff_log_TFP']

model = sm.OLS(y, X).fit(cov_type='HC1')
print(model.summary())





import pandas as pd, numpy as np, statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv('../processed_data/three_digit_NAICS_bds_tfp.csv')


df = df.sort_values(['Industry', 'year']).copy()
df['tfp_index_bds'] = df.groupby('Industry')['tfp_index_2017'].shift(-1)
df['logTFP']        = np.log(df['tfp_index_bds'])


df['logEntry0']  = np.log(df['estabs_entry'])
df['dlogEntry0'] = df.groupby('Industry')['logEntry0'].diff()

MAX_H = 12 # forecast horizon
for h in range(1, MAX_H + 1):
    df[f'F{h}_logTFP'] = df.groupby('Industry')['logTFP'].shift(-h)

df['L0_logTFP'] = df['logTFP']
df['L1_logTFP'] = df.groupby('Industry')['logTFP'].shift(1)

irf, lo90, hi90, lo68, hi68 = [], [], [], [], []

for h in range(1, MAX_H + 1):
    dep   = f'F{h}_logTFP'
    keep  = [dep, 'dlogEntry0', 'L0_logTFP', 'L1_logTFP', 'Industry', 'year']
    dm    = df.dropna(subset=keep)

    formula = (f'{dep} ~ dlogEntry0 + L0_logTFP + L1_logTFP '
               '+ C(year) + C(Industry)')

    mod = smf.ols(formula, data=dm).fit(
            cov_type='cluster',
            cov_kwds={'groups': dm['Industry']}
          )

    beta  = mod.params['dlogEntry0']
    se    = mod.bse['dlogEntry0']

    irf.append(beta)
    hi68.append(beta + 1.00*se); lo68.append(beta - 1.00*se)
    hi90.append(beta + 1.65*se); lo90.append(beta - 1.65*se)

irf_df = pd.DataFrame({
    'horizon': range(1, MAX_H + 1),
    'IRF': irf,
    'lo68': lo68, 'hi68': hi68,
    'lo90': lo90, 'hi90': hi90
})

print(irf_df.round(4))


plt.figure(figsize=(8, 5))

# 90% confidence band
plt.fill_between(
    irf_df['horizon'],
    irf_df['lo90'],
    irf_df['hi90'],
    alpha=0.2,
    label='90% CI'
)

# 68% confidence band
plt.fill_between(
    irf_df['horizon'],
    irf_df['lo68'],
    irf_df['hi68'],
    alpha=0.4,
    label='68% CI'
)

plt.plot(
    irf_df['horizon'],
    irf_df['IRF'],
    label='IRF'
)

plt.axhline(0, linestyle='--', linewidth=1)

plt.xlabel('Years After Entry Shock')
plt.ylabel('Response of LogTFP')
plt.title('IndustryLevel IRF of LogTFP to Entry Shock')
plt.legend()
plt.tight_layout()
plt.show()


sectors = df['Industry'].unique()  
all_irfs = []

for sec in sectors:
    dfi = df[df['Industry'] == sec].copy()

    irf, lo68, hi68, lo90, hi90 = [], [], [], [], []
    for h in range(1, 13):
        dep = f'F{h}_logTFP'
        keep = [dep, 'dlogEntry0', 'L0_logTFP', 'L1_logTFP']
        dm = dfi.dropna(subset=keep + ['year'])

        mod = smf.ols(
            f"{dep} ~ dlogEntry0 + L0_logTFP + L1_logTFP + C(year)",
            data=dm
        ).fit(
            cov_type='cluster',
            cov_kwds={'groups': dm['year']}
        )

        b, s = mod.params['dlogEntry0'], mod.bse['dlogEntry0']
        irf.append(b)
        lo68.append(b - 1 * s)
        hi68.append(b + 1 * s)
        lo90.append(b - 1.65 * s)
        hi90.append(b + 1.65 * s)

    tmp = pd.DataFrame({
        'Industry': sec,
        'horizon': range(1, 13),
        'IRF': irf,
        'lo68': lo68, 'hi68': hi68,
        'lo90': lo90, 'hi90': hi90
    })
    all_irfs.append(tmp)

irf_sector_df = pd.concat(all_irfs, ignore_index=True)

g = sns.FacetGrid(irf_sector_df,
                  col="Industry", col_wrap=4, sharey=True, height=3)
g.map_dataframe(sns.lineplot, "horizon", "IRF", color="black")
g.map_dataframe(plt.fill_between, "horizon", "lo90", "hi90",
                alpha=0.2, color="gray")
g.map_dataframe(plt.fill_between, "horizon", "lo68", "hi68",
                alpha=0.4, color="gray")
for ax in g.axes.flatten():
    ax.axhline(0, ls="--", c="red")
    ax.set_xlabel("Years After Shock")
    ax.set_ylabel("Log-TFP Response")
plt.tight_layout()
plt.show()





import pandas as pd


filepath = '../processed_data/two_digit_NAICS_bds_tfp.csv'

df = pd.read_csv(filepath)
df.head()


df.columns


df.dtypes


'''
which way round is the causality? You would think to have business dynamism on the RHS as a proxy for creative destruction
'''


# Business dynamism measures
 # 'job_creation_rate', 'job_destruction_rate'
bus_vars = [ 'reallocation_rate', 'estabs_entry_rate', 'estabs_exit_rate']

# generate lags
for v in bus_vars:
    df[f"{v}_lag"] = df.groupby("NAICS")[v].shift(1)


# Convert tfp_pct_change to numeric, replacing non-numeric values with NaN
df['tfp_pct_change'] = pd.to_numeric(df['tfp_pct_change'], errors='coerce')

# Drop rows where tfp_pct_change is NaN
df_clean = df.dropna(subset=['tfp_pct_change'])


df_clean.head()


import statsmodels.formula.api as smf

formula = (
    "tfp_pct_change ~ " +
    " + ".join(bus_vars) +                       # contemporaneous
    " + " + " + ".join([v+"_lag" for v in bus_vars]) +  # lagged
    " + C(NAICS) + C(year)"                      # industry & year FE
)

fe_mod = smf.ols(formula=formula, data=df_clean).fit(
    cov_type="cluster", cov_kwds={"groups": df_clean["year"]}
)
print(fe_mod.summary())


